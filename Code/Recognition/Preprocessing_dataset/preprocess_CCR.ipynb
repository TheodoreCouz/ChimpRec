{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "#path to append: \"./ChimpRec/Code\"\n",
    "sys.path.append(...)\n",
    "\n",
    "from chimplib.imports import pd, cv2, os, random, np, Image, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the folder with CCR videos\n",
    "videos_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/ChimpRec-Dataset/CCR/data/videos\"\n",
    "#path to file with CCR face annotations (face_data.csv)\n",
    "annotations_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/ChimpRec-Dataset/CCR/annotations/face_data.csv\"\n",
    "#path to folder where dataset is to be created\n",
    "dataset_path = \"C:/Users/julie/Documents/Unif/Mémoire/CCR_recognition_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# video_path:  path to the video file\n",
    "# @output:\n",
    "# (width, height): width and height of the video in pixels\n",
    "def get_video_resolution(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    return width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# df: pd.DataFrame of the face.csv file of the CCR dataset\n",
    "# exclude_list: optional list of individual names to exclude\n",
    "# @output:\n",
    "# individuals: list of individuals (labels) in the dataset but not in exclude_list\n",
    "def get_individuals(df, exclude_list=None):\n",
    "    individuals = list(df['label'].unique())\n",
    "    for name in exclude_list or []:\n",
    "        if name in individuals:\n",
    "            individuals.remove(name)\n",
    "    return individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f38a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# df: pd.DataFrame of the face.csv file of the CCR dataset\n",
    "# videos_path: path to the folder containing the videos of the CCR dataset\n",
    "# @output:\n",
    "# resolutions: dictionary mapping each video name to its (width, height) resolution\n",
    "def compute_video_resolutions(df, videos_path):\n",
    "    resolutions = {}\n",
    "    for video in df['video'].unique():\n",
    "        year = 2012 if video in os.listdir(os.path.join(videos_path, \"2012\")) else 2013\n",
    "        resolutions[video] = get_video_resolution(os.path.join(videos_path, str(year), video))\n",
    "    return resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# df: pd.DataFrame of the face.csv file of the CCR dataset\n",
    "# individual: list of individuals (labels) to process\n",
    "# videos_resolution: dictionary mapping video names to their resolutions (width, height))\n",
    "# @output:\n",
    "# dict - split dataframes {'train': df_train, 'val': df_val, 'test': df_test}\n",
    "#         where each dataframe contains only frames for the given individual,\n",
    "#         and only if the crop size is >100x100 pixels\n",
    "def split_and_filter_individual_data(df, individual, videos_resolution):\n",
    "    videos = list(df[df['label'] == individual]['video'].unique())\n",
    "    random.shuffle(videos)\n",
    "\n",
    "    #Divides videos into 70% training set, 15% validation and 15% test\n",
    "    train_videos = videos[:int(0.7 * len(videos))]\n",
    "    val_videos = videos[int(0.7 * len(videos)):int(0.85 * len(videos))]\n",
    "    test_videos = videos[int(0.85 * len(videos)):]\n",
    "\n",
    "    df_individual = df[df['label'] == individual].copy()\n",
    "    valid_rows = []\n",
    "    for _, row in df_individual.iterrows():\n",
    "        width, height = videos_resolution[row[\"video\"]]\n",
    "        crop_width = row['w'] * width\n",
    "        crop_height = row['h'] * height\n",
    "        #checks that the resolution respects the minimum threshold\n",
    "        if crop_width > 100 and crop_height > 100:\n",
    "            valid_rows.append(row)\n",
    "    df_filtered = pd.DataFrame(valid_rows)\n",
    "\n",
    "    train_subset = df_filtered[df_filtered['video'].isin(train_videos)]\n",
    "    val_subset = df_filtered[df_filtered['video'].isin(val_videos)]\n",
    "    test_subset = df_filtered[df_filtered['video'].isin(test_videos)]\n",
    "\n",
    "    #If there are not enough good quality frames in one of the sets, we complete the set with frames intended for \n",
    "    # the training set and remove these frames from the training set\n",
    "    if len(val_subset) < 250: \n",
    "        extra_val_images = train_subset.sample(n=250-len(val_subset), random_state=42, replace=False)\n",
    "        val_subset = pd.concat([val_subset, extra_val_images])\n",
    "        train_subset = train_subset.drop(extra_val_images.index) \n",
    "\n",
    "    if len(test_subset) < 250: \n",
    "        extra_test_images = train_subset.sample(n=250-len(test_subset), random_state=42, replace=False)\n",
    "        test_subset = pd.concat([test_subset, extra_test_images])\n",
    "        train_subset = train_subset.drop(extra_test_images.index) \n",
    "\n",
    "    return {\n",
    "        \"train\": train_subset.sample(n=1000, random_state=42, replace=False),\n",
    "        \"val\": val_subset.sample(n=250, random_state=42, replace=False),\n",
    "        \"test\": test_subset.sample(n=250, random_state=42, replace=False)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6791f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# data: dictionary mapping individual names to his pd.DataFrame\n",
    "# split: \"train\", \"val\", or \"test\"\n",
    "# videos_path: path to the folder containing the videos of the CCR dataset\n",
    "# dataset_path: path to the folder where cropped face images will be saved\n",
    "# @output:\n",
    "# None (cropped face images are saved in corresponding directories)\n",
    "def save_cropped_faces(data, split, videos_path, dataset_path):\n",
    "    for individual, frames in data.items():\n",
    "        split_path = os.path.join(dataset_path, split)\n",
    "\n",
    "        #If you're on a train or validating a set, separate the images into individual folders\n",
    "        if split in [\"train\", \"val\"]:\n",
    "            individual_path = os.path.join(split_path, individual)\n",
    "            os.makedirs(individual_path, exist_ok=True)\n",
    "        else:\n",
    "            individual_path = split_path\n",
    "\n",
    "        id  = 0\n",
    "\n",
    "        for idx, row in frames.iterrows():\n",
    "            video_path = os.path.join(videos_path, str(row[\"year\"]), row[\"video\"])\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "            if not cap.isOpened():\n",
    "                print(f\"+Erreur ouverture vidéo : {row['video']}\")\n",
    "                continue\n",
    "            \n",
    "            #retrieve the frame of interest\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, row[\"frame\"])\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret or frame is None:\n",
    "                print(f\"Impossible de lire la frame {row['frame']} de {row['video']}\")\n",
    "                cap.release()\n",
    "                continue\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            x1 = int(row[\"x\"] * w)\n",
    "            y1 = int(row[\"y\"] * h)\n",
    "            x2 = x1 + int(row[\"w\"] * w)\n",
    "            y2 = y1 + int(row[\"h\"] * h)\n",
    "\n",
    "            # Correction if the bbox exceed the edges of the image\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w, x2), min(h, y2)\n",
    "\n",
    "            face_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "            #check if crop is empty\n",
    "            if face_crop is None or face_crop.size == 0:\n",
    "                print(f\"face_crop vide pour {row['video']} frame {row['frame']}\")\n",
    "                continue\n",
    "\n",
    "            # converts images to uint8 type, which is the expected standard format for images in libraries such as PIL\n",
    "            face_crop = face_crop.astype(np.uint8)  \n",
    "\n",
    "            filename = f\"{individual}_{id}.jpg\"\n",
    "            id += 1\n",
    "\n",
    "            filepath = os.path.join(individual_path, filename)\n",
    "            image_pil = Image.fromarray(np.uint8(face_crop))\n",
    "            image_pil.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21394e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_path = os.path.join(dataset_path, split)\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(annotations_path)\n",
    "#Not these labels (['NEGATIVE', 'VELU', 'PAMA', 'YO']) because either not an individual or not enough data for the individual\n",
    "individuals = get_individuals(df, exclude_list=['NEGATIVE', 'VELU', 'PAMA', 'YO'])\n",
    "videos_resolution = compute_video_resolutions(df, videos_path)\n",
    "split_datas = split_and_filter_individual_data(df, individuals, videos_resolution)\n",
    "\n",
    "data_split = {}\n",
    "for individual in individuals:\n",
    "    data_split[individual] = split_and_filter_individual_data(df, individual, videos_resolution)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(dataset_path, split), exist_ok=True)\n",
    "    subset = {}\n",
    "    for individual, splits in data_split.items():\n",
    "        subset[individual] = splits[split]\n",
    "\n",
    "    save_cropped_faces(subset, split, videos_path, dataset_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
