{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161b7bb5",
   "metadata": {},
   "source": [
    "<h2>File paths and imports<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539e4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchreid\\reid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n",
      "c:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code\")\n",
    "\n",
    "from chimplib.imports import *\n",
    "from chimplib.recognition_utils import *\n",
    "\n",
    "#Path to the root of the ChimpRec database \".../Chimprec Dataset/Datasets/Face Recognition/ChimpRec\"\n",
    "chimprec_dataset_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Datasets/Face Recognition/ChimpRec\"\n",
    "chimprec_train_dataset = f\"{chimprec_dataset_path}/train\"\n",
    "chimprec_val_dataset = f\"{chimprec_dataset_path}/val\"\n",
    "chimprec_test_dataset = f\"{chimprec_dataset_path}/test\"\n",
    "#Path to the root of the ChimpRec model \".../Chimprec Dataset/Models/recognition/ChimpRec\"\n",
    "chimprec_models_tl_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Models/recognition/ChimpRec/facenet_16_layers_triplet_loss.pth\"\n",
    "\n",
    "#Path to the root of the CCR database \".../Chimprec Dataset/Datasets/Face Recognition/CCR\"\n",
    "ccr_dataset_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Datasets/Face Recognition/CCR\"\n",
    "ccr_train_dataset = f\"{ccr_dataset_path}/train\"\n",
    "ccr_val_dataset = f\"{ccr_dataset_path}/val\"\n",
    "ccr_test_dataset = f\"{ccr_dataset_path}/test\"\n",
    "#Path to the root of the CCR model \".../Chimprec Dataset/Models/recognition/CCR\"\n",
    "#ccr_models_tl_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Models/recognition/CCR/facenet_13_layers_triplet_loss.pth\"\n",
    "ccr_models_tl_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code/Recognition/FaceNet/Triplet_loss/facenet_13_layers_triplet_loss.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96eb82f",
   "metadata": {},
   "source": [
    "<h2>Utils<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111ee82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation to be applied to network input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca3a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# model_path: path to the saved model file (.pth file) containing weights\n",
    "# @output:\n",
    "# model: FaceNet model loaded with the specified weights\n",
    "def load_model(model_path):\n",
    "    model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc6a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# train_path: path to the training dataset\n",
    "# val_path: path to the validation dataset\n",
    "# model: the model used to compute embeddings\n",
    "# transform: set of transformations to apply to images (resize, normalize, ...) with torchvision.transforms\n",
    "# has_face_labels: boolean indicating whether face labels (.txt files) are used for cropping\n",
    "# @output:\n",
    "# all_embeddings: dictionary {class_name: [embedding vectors]} with embeddings of the validation and train set images linked to their class\n",
    "def merge_all_embeddings(train_path, val_path, model, transform, has_face_labels):\n",
    "    emb1 = get_all_embeddings(train_path, model, device, transform, has_face_labels)\n",
    "    emb2 = get_all_embeddings(val_path, model, device, transform, has_face_labels)\n",
    "    all_embeddings = {}\n",
    "    for key in emb1.keys():\n",
    "        all_embeddings[key] = emb1.get(key, []) + emb2.get(key, [])\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# test_dataset: path to the test dataset\n",
    "# model: model used for prediction\n",
    "# all_embeddings: dictionary {class_name: [embedding vectors]} from the validation and train set for nearest neighbor comparison\n",
    "# label_fn: function that extracts the ground-truth identity from the image filename\n",
    "# @output:\n",
    "# accuracy: float value representing the proportion of correctly classified images in the test dataset\n",
    "def evaluate_model(test_dataset, model, all_embeddings, label_fn):\n",
    "    correctly_classified = 0\n",
    "    total_img = 0\n",
    "\n",
    "    for img in os.listdir(test_dataset):\n",
    "        img_path = os.path.join(test_dataset, img)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        predicted_label = predict_face_with_tl(image, model, all_embeddings, device, transform, 1)\n",
    "        real_label = label_fn(img)\n",
    "        if predicted_label == real_label:\n",
    "            correctly_classified += 1\n",
    "        total_img += 1\n",
    "\n",
    "    return correctly_classified / total_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee20dab",
   "metadata": {},
   "source": [
    "<h2>Evaluation on the test set<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb18893",
   "metadata": {},
   "source": [
    "<h3>ChimpRec<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9259c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PD\n",
      "AD\n",
      "PD\n",
      "AD\n",
      "IV\n",
      "AD\n",
      "PD\n",
      "AD\n",
      "PD\n",
      "AD\n",
      "AD\n",
      "AD\n",
      "BL\n",
      "AD\n",
      "LM\n",
      "BL\n",
      "PD\n",
      "BL\n",
      "PD\n",
      "BL\n",
      "PD\n",
      "BL\n",
      "PD\n",
      "BL\n",
      "LM\n",
      "BL\n",
      "KG\n",
      "BL\n",
      "MM\n",
      "BS\n",
      "PD\n",
      "BS\n",
      "MM\n",
      "BS\n",
      "MM\n",
      "BS\n",
      "AD\n",
      "BS\n",
      "KR\n",
      "BS\n",
      "PD\n",
      "BS\n",
      "NJ\n",
      "BS\n",
      "KR\n",
      "DK\n",
      "KR\n",
      "DK\n",
      "KR\n",
      "DK\n",
      "LM\n",
      "DK\n",
      "PD\n",
      "DK\n",
      "LM\n",
      "DK\n",
      "DK\n",
      "DK\n",
      "IV\n",
      "IV\n",
      "IV\n",
      "IV\n",
      "LM\n",
      "IV\n",
      "PD\n",
      "IV\n",
      "LM\n",
      "IV\n",
      "LM\n",
      "IV\n",
      "LM\n",
      "IV\n",
      "MG\n",
      "JJ\n",
      "MG\n",
      "JJ\n",
      "MK\n",
      "JJ\n",
      "MK\n",
      "JJ\n",
      "PD\n",
      "JJ\n",
      "PD\n",
      "JJ\n",
      "PD\n",
      "JJ\n",
      "LM\n",
      "JJ\n",
      "AD\n",
      "KG\n",
      "AD\n",
      "KG\n",
      "AD\n",
      "KG\n",
      "AD\n",
      "KG\n",
      "AD\n",
      "KG\n",
      "NJ\n",
      "KM\n",
      "NJ\n",
      "KM\n",
      "AD\n",
      "KM\n",
      "PD\n",
      "KM\n",
      "PD\n",
      "KM\n",
      "PD\n",
      "KM\n",
      "LM\n",
      "KM\n",
      "PD\n",
      "LM\n",
      "LM\n",
      "LM\n",
      "PD\n",
      "LM\n",
      "PD\n",
      "LM\n",
      "PD\n",
      "LM\n",
      "PD\n",
      "LM\n",
      "TT\n",
      "LM\n",
      "LM\n",
      "LM\n",
      "TT\n",
      "LM\n",
      "IV\n",
      "MG\n",
      "LM\n",
      "MG\n",
      "LM\n",
      "MG\n",
      "MG\n",
      "MG\n",
      "MG\n",
      "MG\n",
      "MG\n",
      "MG\n",
      "DK\n",
      "MG\n",
      "LM\n",
      "MG\n",
      "PD\n",
      "MG\n",
      "AD\n",
      "MK\n",
      "AD\n",
      "MK\n",
      "AD\n",
      "MK\n",
      "LM\n",
      "MK\n",
      "LM\n",
      "MK\n",
      "LM\n",
      "MZ\n",
      "LM\n",
      "MZ\n",
      "LM\n",
      "MZ\n",
      "LM\n",
      "MZ\n",
      "LM\n",
      "MZ\n",
      "LM\n",
      "MZ\n",
      "LM\n",
      "MZ\n",
      "PD\n",
      "MZ\n",
      "DK\n",
      "NJ\n",
      "LM\n",
      "NJ\n",
      "LM\n",
      "NJ\n",
      "DK\n",
      "NJ\n",
      "PD\n",
      "NJ\n",
      "PD\n",
      "NJ\n",
      "PD\n",
      "NJ\n",
      "PD\n",
      "NJ\n",
      "LM\n",
      "NJ\n",
      "LM\n",
      "NR\n",
      "MM\n",
      "NR\n",
      "MZ\n",
      "NR\n",
      "NR\n",
      "NR\n",
      "MZ\n",
      "NR\n",
      "MM\n",
      "NR\n",
      "MM\n",
      "NR\n",
      "LM\n",
      "PD\n",
      "PD\n",
      "PD\n",
      "PD\n",
      "PD\n",
      "PD\n",
      "PD\n",
      "LM\n",
      "PD\n",
      "PD\n",
      "PD\n",
      "KR\n",
      "PD\n",
      "LM\n",
      "TC\n",
      "PD\n",
      "TC\n",
      "LM\n",
      "TC\n",
      "TC\n",
      "TC\n",
      "PD\n",
      "TC\n",
      "MM\n",
      "TC\n",
      "TT\n",
      "TC\n",
      "LM\n",
      "TS\n",
      "LM\n",
      "TS\n",
      "PD\n",
      "TS\n",
      "LM\n",
      "TS\n",
      "PD\n",
      "TS\n",
      "MK\n",
      "TS\n",
      "MK\n",
      "TS\n",
      "LM\n",
      "TT\n",
      "LM\n",
      "TT\n",
      "LM\n",
      "TT\n",
      "TT\n",
      "TT\n",
      "LM\n",
      "TT\n",
      "LM\n",
      "TT\n",
      "LM\n",
      "TT\n",
      "PD\n",
      "TT\n",
      "ChimpRec Accuracy: 0.12121212121212122\n"
     ]
    }
   ],
   "source": [
    "#To switch from the file name (for example: “AD1.jpg”) to its identity (for example: “AD”)\n",
    "chimprec_label_fn = lambda img: img[:2] \n",
    "\n",
    "chimprec_model = load_model(chimprec_models_tl_path)\n",
    "all_embeddings = merge_all_embeddings(chimprec_train_dataset, chimprec_val_dataset, chimprec_model, transform, True)\n",
    "accuracy = evaluate_model(chimprec_test_dataset, chimprec_model, all_embeddings, chimprec_label_fn)\n",
    "print(\"ChimpRec Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cf271",
   "metadata": {},
   "source": [
    "<h3>CCR<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede34326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCR Accuracy: 0.6304\n"
     ]
    }
   ],
   "source": [
    "#To switch from the file name (for example: “FANA_0.jpg”) to its identity (for example: “FANA”)\n",
    "ccr_label_fn = lambda img: img.split(\"_\")[0]\n",
    "\n",
    "ccr_model = load_model(ccr_models_tl_path)\n",
    "all_embeddings = merge_all_embeddings(ccr_train_dataset, ccr_val_dataset, ccr_model, transform, False)\n",
    "accuracy = evaluate_model(ccr_test_dataset, ccr_model, all_embeddings, ccr_label_fn)\n",
    "print(\"CCR Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
