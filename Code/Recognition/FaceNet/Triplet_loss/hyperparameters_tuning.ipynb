{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcb9798",
   "metadata": {},
   "source": [
    "<h2>File paths and imports<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code\")\n",
    "\n",
    "from chimplib.imports import *\n",
    "from chimplib.recognition_utils import *\n",
    "\n",
    "#Path to the root of the ChimpRec database \".../Chimprec Dataset/Datasets/Face Recognition/ChimpRec\"\n",
    "chimprec_dataset_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Datasets/Face Recognition/ChimpRec\"\n",
    "#Path to store the different models based on the triplet loss for ChimpRec (typically \"./Code/recognition/FaceNet/Models/ChimpRec_triplet_loss\")\n",
    "chimprec_models_tl_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code/recognition/FaceNet/Models/ChimpRec_triplet_loss\"\n",
    "\n",
    "#Path to the root of the CCR database\".../Chimprec Dataset/Datasets/Face Recognition/CCR\"\n",
    "ccr_dataset_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Datasets/Face Recognition/CCR\"\n",
    "#Path to store the different models based on the triplet loss for CCR (typically \"./Code/recognition/FaceNet/Models/CCR_triplet_loss\")\n",
    "ccr_models_tl_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code/recognition/FaceNet/Models/CCR_triplet_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db944917",
   "metadata": {},
   "source": [
    "<h2>Utils<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation to be applied to network input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "layers_to_test = [5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "k_to_test = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1af4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# model: the model used for inference\n",
    "# val_dataset_path: path to the validation dataset directory\n",
    "# transform: set of transformations to apply to images (resize, normalize, ...) with torchvision.transforms\n",
    "# k_vals: list of k values to evaluate (for example : [1, 3, 5])\n",
    "# class_names: list of class names (individual identities) corresponding to the index labels\n",
    "# all_embeddings: dictionary {class_name: [embedding vectors]} to which a new image is compared during k-NN\n",
    "# has_face_labels: boolean indicating whether face labels (.txt files) are used for cropping\n",
    "# @output:\n",
    "# precisions: dictionary {k: float} representing the accuracy for each value of k on the validation set\n",
    "def evaluate_model_on_val(model, val_dataset_path, transform, k_vals, class_names, all_embeddings, has_face_labels):\n",
    "    precisions = {k: 0 for k in k_vals}\n",
    "    total_img = 0\n",
    "\n",
    "    for indiv in os.listdir(val_dataset_path):\n",
    "        #The dataset structure changes depending on whether the images are already crop or if the bboxes are written to files\n",
    "        indiv_path = os.path.join(val_dataset_path, indiv, \"images\") if has_face_labels else os.path.join(val_dataset_path, indiv)\n",
    "        for img_name in os.listdir(indiv_path):\n",
    "            total_img += 1\n",
    "            img_path = os.path.join(indiv_path, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if has_face_labels:\n",
    "                #Files with annotations have the same name as images, but in txt instead of an image-specific extension (jpg, JPG, png)\n",
    "                label_path = img_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\").replace(\".JPG\", \".txt\").replace(\".png\", \".txt\")\n",
    "                bbox = read_yolo_label(label_path, image.width, image.height)\n",
    "                if bbox:\n",
    "                    image = image.crop(bbox)\n",
    "\n",
    "            for k in k_vals:\n",
    "                pred = predict_face_with_tl(image, model, all_embeddings, device, transform, k, class_names)\n",
    "                if pred == indiv:\n",
    "                    precisions[k] += 1\n",
    "\n",
    "    for k in precisions:\n",
    "        precisions[k] /= total_img\n",
    "    return precisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# dataset_path: path to the root dataset directory\n",
    "# model_folder: path to the directory containing the saved models\n",
    "# has_face_labels: boolean indicating whether face labels (.txt files) are used for cropping\n",
    "# @output:\n",
    "# results: dictionary {layer: {k: precision}} containing the accuracy for each model configuration (k value and number of fine-tuned layer)\n",
    "def run_experiment(dataset_path, model_folder, has_face_labels):\n",
    "    train_path = os.path.join(dataset_path, \"train\")\n",
    "    val_path = os.path.join(dataset_path, \"val\")\n",
    "    \n",
    "    #We recreate the dataset to retrieve the association between index and identity\n",
    "    train_triplet_dataset = TripletLossDataset(train_path, transform=transform)\n",
    "    class_names = list(train_triplet_dataset.class_to_idx.keys())\n",
    "\n",
    "    results = {}\n",
    "    for layer in layers_to_test:\n",
    "        model_path = f\"{model_folder}/facenet_{layer}_layers_triplet_loss.pth\"\n",
    "        model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "        all_embeddings = get_all_embeddings(train_path, model, device, transform, has_face_labels)\n",
    "        precisions = evaluate_model_on_val(model, val_path, transform, k_to_test, class_names, all_embeddings, has_face_labels)\n",
    "        print(layer)\n",
    "        print(precisions)\n",
    "        results[layer] = precisions\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6c381",
   "metadata": {},
   "source": [
    "<h2>Calculating model scores<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de862232",
   "metadata": {},
   "source": [
    "<h3>ChimpRec<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59be46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(chimprec_dataset_path, chimprec_models_tl_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24657d0",
   "metadata": {},
   "source": [
    "<h3>CCR<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21677673",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(ccr_dataset_path, ccr_models_tl_path, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56061d",
   "metadata": {},
   "source": [
    "<h2>Graphs<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef495b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# results: dictionary {layer: {k: precision}} generated by run_experiment\n",
    "# dataset_name: name of the dataset on wich the results have been calculated\n",
    "# file_save_path: path where the accuracy plot image should be saved\n",
    "# @output:\n",
    "# None (saves and displays a plot of accuracy for different hyperparameter configurations)\n",
    "def show_graph(results, dataset_name, file_save_path):\n",
    "    layers = list(results.keys())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for k in k_to_test:\n",
    "        values = [results[layer][k] for layer in layers]\n",
    "        plt.plot(layers, values, marker='o', label=f'k = {k}')\n",
    "\n",
    "    plt.xlabel('#Fine-tuned layers')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Evolution of the accuracy on the {dataset_name} validation dataset according to the value of the hyperparameters')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000e772",
   "metadata": {},
   "source": [
    "<h3>ChimpRec<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7befdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values obtained when calculating accuracy for all configurations with the ChimpRec model (to avoid having to rerun all the code each time)\n",
    "chimprec_results = {5: {1: 0.7533039647577092, 3: 0.7180616740088106, 5: 0.7136563876651982}, 6: {1: 0.8810572687224669, 3: 0.8854625550660793, 5: 0.8854625550660793}, 7: {1: 0.8193832599118943, 3: 0.7929515418502202, 5: 0.8193832599118943}, 8: {1: 0.8546255506607929, 3: 0.8502202643171806, 5: 0.8414096916299559}, 9: {1: 0.8193832599118943, 3: 0.8193832599118943, 5: 0.8237885462555066}, 10: {1: 0.7048458149779736, 3: 0.7092511013215859, 5: 0.7136563876651982}, 11: {1: 0.8061674008810573, 3: 0.8105726872246696, 5: 0.801762114537445}, 12: {1: 0.8986784140969163, 3: 0.8722466960352423, 5: 0.8766519823788547}, 13: {1: 0.8810572687224669, 3: 0.8898678414096917, 5: 0.8898678414096917}, 14: {1: 0.8061674008810573, 3: 0.8105726872246696, 5: 0.8193832599118943}, 15: {1: 0.8634361233480177, 3: 0.8502202643171806, 5: 0.8590308370044053}, 16: {1: 0.9118942731277533, 3: 0.9030837004405287, 5: 0.8898678414096917}, 17: {1: 0.7577092511013216, 3: 0.7444933920704846, 5: 0.7577092511013216}, 18: {1: 0.8414096916299559, 3: 0.8502202643171806, 5: 0.8502202643171806}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(chimprec_results, \"ChimpRec\", \"C:/Users/julie/Documents/Unif/Test_mémoire/fine_tuning_ChimpRec_triplet_loss.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629a328",
   "metadata": {},
   "source": [
    "<h3>CCR<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values obtained when calculating accuracy for all configurations with the CCR model (to avoid having to rerun all the code each time)\n",
    "ccr_results = {5: {1: 0.296, 3: 0.3028, 5: 0.3116}, 6: {1: 0.4876, 3: 0.5016, 5: 0.5088}, 7: {1: 0.4252, 3: 0.4424, 5: 0.4452}, 8: {1: 0.6112, 3: 0.6108, 5: 0.6152}, 9: {1: 0.4064, 3: 0.4104, 5: 0.4224}, 10: {1: 0.3344, 3: 0.3484, 5: 0.3548}, 11: {1: 0.5004, 3: 0.5068, 5: 0.5016}, 12: {1: 0.5428, 3: 0.542, 5: 0.5416}, 13: {1: 0.636, 3: 0.632, 5: 0.6332}, 14: {1: 0.5276, 3: 0.5352, 5: 0.5352}, 15: {1: 0.52, 3: 0.5236, 5: 0.5192}, 16: {1: 0.5096, 3: 0.5176, 5: 0.5276}, 17: {1: 0.4068, 3: 0.4188, 5: 0.42}, 18: {1: 0.4576, 3: 0.4596, 5: 0.4604}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a181c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(ccr_results, \"CCR\", \"C:/Users/julie/Documents/Unif/Test_mémoire/fine_tuning_CCR_triplet_loss.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
