{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161b7bb5",
   "metadata": {},
   "source": [
    "<h2>File paths and imports<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539e4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code\")\n",
    "\n",
    "from chimplib.imports import *\n",
    "from chimplib.recognition_utils import *\n",
    "\n",
    "#Path to the root of the ChimpRec database \".../Chimprec Dataset/Datasets/Face Recognition/ChimpRec\"\n",
    "chimprec_dataset_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Datasets/Face Recognition/ChimpRec\"\n",
    "chimprec_train_dataset = f\"{chimprec_dataset_path}/train\"\n",
    "chimprec_test_dataset = f\"{chimprec_dataset_path}/test\"\n",
    "#Path to the root of the ChimpRec model \".../Chimprec Dataset/Models/recognition/ChimpRec\"\n",
    "chimprec_models_fc_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Models/recognition/ChimpRec/facenet_14_layers_fc.pth\"\n",
    "\n",
    "#Path to the root of the CCR database \".../Chimprec Dataset/Datasets/Face Recognition/CCR\"\n",
    "ccr_dataset_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Datasets/Face Recognition/CCR\"\n",
    "ccr_train_dataset = f\"{ccr_dataset_path}/train\"\n",
    "ccr_test_dataset = f\"{ccr_dataset_path}/test\"\n",
    "#Path to the root of the CCR model \".../Chimprec Dataset/Models/recognition/CCR\"\n",
    "#ccr_models_fc_path = \"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/Chimprec Dataset/Models/recognition/CCR/facenet_13_layers_triplet_loss.pth\"\n",
    "ccr_models_fc_path = \"C:/Users/julie/Documents/Unif/Test_mémoire/FCL_models_CCR3/facenet_15_layers_fc.pth\" #\"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code/Recognition/FaceNet/Fully_connected_layer/facenet_14_layers_fc.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96eb82f",
   "metadata": {},
   "source": [
    "<h2>Utils<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111ee82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation to be applied to network input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca3a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# model_path: path to the saved model file (.pth file) containing weights\n",
    "# dataset_train_path: path to the training dataset\n",
    "# device: torch device ('cpu' or 'cuda') to perform computation\n",
    "# has_faces_label: boolean indicating whether face labels (.txt files) are used for cropping\n",
    "# @outputs:\n",
    "# model: the trained model loaded and ready for inference\n",
    "# class_names: list of class names corresponding to identity labels\n",
    "def load_model(model_path, dataset_train_path, device, has_label_face):\n",
    "    num_classes = len(os.listdir(dataset_train_path))\n",
    "    model = CustomFCLFaceNet(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    train_dataset = FullyConnectedLayerDataset(dataset_train_path, transform=transform, has_faces_label=has_label_face)\n",
    "    class_names = list(train_dataset.class_to_idx.keys())\n",
    "\n",
    "    return model, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be0377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# class_names: list of class names corresponding to identity labels\n",
    "# test_set_path: path to the test dataset\n",
    "# model: model used for prediction\n",
    "# label_fn: function that extracts the ground-truth identity from the image filename\n",
    "# has_face_label: boolean indicating whether face label files (.txt) are used for cropping (not used in this function)\n",
    "# @output:\n",
    "# accuracy: float value representing the proportion of correctly classified images in the test dataset\n",
    "def evaluate_model(class_names, test_set_path, model, label_fn, has_face_label):\n",
    "    correctly_classified = 0\n",
    "    imgs = os.listdir(test_set_path)\n",
    "    for img in imgs:\n",
    "        img_path = os.path.join(test_set_path, img)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        predicted_label, _ = predict_face_with_fc(image, model, transform, device)\n",
    "        predicted_identity = class_names[predicted_label]\n",
    "\n",
    "        real_identity = label_fn(img)\n",
    "        if predicted_identity == real_identity:\n",
    "            correctly_classified += 1\n",
    "\n",
    "    accuracy = correctly_classified / len(imgs)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee20dab",
   "metadata": {},
   "source": [
    "<h2>Evaluation on the test set<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb18893",
   "metadata": {},
   "source": [
    "<h3>ChimpRec<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9259c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "AD1.png\n",
      "AD2.png\n",
      "AD3.png\n",
      "AD4.png\n",
      "AD5.png\n",
      "AD6.png\n",
      "AD7.png\n",
      "BL1.png\n",
      "BL3.png\n",
      "BL5.png\n",
      "BL6.png\n",
      "BL7.png\n",
      "BL8.png\n",
      "BL9.png\n",
      "BS1.png\n",
      "BS10.png\n",
      "BS2.png\n",
      "BS4.png\n",
      "BS5.png\n",
      "BS7.png\n",
      "BS8.png\n",
      "BS9.png\n",
      "DK1.png\n",
      "DK4.png\n",
      "DK5.png\n",
      "DK6.png\n",
      "DK7.png\n",
      "DK8.png\n",
      "DK9.png\n",
      "IV2.png\n",
      "IV3.png\n",
      "IV4.png\n",
      "IV6.png\n",
      "IV7.png\n",
      "IV8.png\n",
      "IV9.png\n",
      "JJ1.png\n",
      "JJ2.png\n",
      "JJ4.png\n",
      "JJ5.png\n",
      "JJ6.png\n",
      "JJ7.png\n",
      "JJ8.png\n",
      "JJ9.png\n",
      "KG1.png\n",
      "KG2.png\n",
      "KG3.png\n",
      "KG4.png\n",
      "KG5.png\n",
      "KM1.png\n",
      "KM2.png\n",
      "KM3.png\n",
      "KM4.png\n",
      "KM5.png\n",
      "KM6.png\n",
      "KM7.png\n",
      "LM1.png\n",
      "LM2.png\n",
      "LM3.png\n",
      "LM4.png\n",
      "LM5.png\n",
      "LM6.png\n",
      "LM7.png\n",
      "LM8.png\n",
      "LM9.png\n",
      "MG1.png\n",
      "MG11.png\n",
      "MG13.png\n",
      "MG2.png\n",
      "MG3.png\n",
      "MG4.png\n",
      "MG5.png\n",
      "MG7.png\n",
      "MG8.png\n",
      "MK1.png\n",
      "MK2.png\n",
      "MK3.png\n",
      "MK4.png\n",
      "MK5.png\n",
      "MZ1.png\n",
      "MZ2.png\n",
      "MZ3.png\n",
      "MZ4.png\n",
      "MZ5.png\n",
      "MZ6.png\n",
      "MZ7.png\n",
      "MZ8.png\n",
      "NJ10.png\n",
      "NJ11.png\n",
      "NJ12.png\n",
      "NJ13.png\n",
      "NJ3.png\n",
      "NJ4.png\n",
      "NJ6.png\n",
      "NJ7.png\n",
      "NJ8.png\n",
      "NR.png\n",
      "NR1.png\n",
      "NR2.png\n",
      "NR3.png\n",
      "NR4.png\n",
      "NR5.png\n",
      "NR7.png\n",
      "PD1.png\n",
      "PD2.png\n",
      "PD3.png\n",
      "PD4.png\n",
      "PD5.png\n",
      "PD7.png\n",
      "PD8.png\n",
      "TC2.png\n",
      "TC3.png\n",
      "TC5.png\n",
      "TC6.png\n",
      "TC7.png\n",
      "TC8.png\n",
      "TC9.png\n",
      "TS1.png\n",
      "TS2.png\n",
      "TS3.png\n",
      "TS4.png\n",
      "TS5.png\n",
      "TS6.png\n",
      "TS7.png\n",
      "TT1.png\n",
      "TT3.png\n",
      "TT4.png\n",
      "TT5.png\n",
      "TT6.png\n",
      "TT7.png\n",
      "TT8.png\n",
      "TT9.png\n",
      "ChimpRec Accuracy: 0.1364\n"
     ]
    }
   ],
   "source": [
    "#To switch from the file name (for example: “AD1.jpg”) to its identity (for example: “AD”)\n",
    "chimprec_label_fn = lambda img: img[:2] \n",
    "\n",
    "chimprec_model, chimprec_class_names = load_model(chimprec_models_fc_path, chimprec_train_dataset, device, True)\n",
    "chimprec_accuracy = evaluate_model(chimprec_class_names, chimprec_test_dataset, chimprec_model, chimprec_label_fn)\n",
    "print(f\"ChimpRec Accuracy: {chimprec_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cf271",
   "metadata": {},
   "source": [
    "<h3>CCR<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede34326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCR Accuracy: 0.6464\n"
     ]
    }
   ],
   "source": [
    "#To switch from the file name (for example: “FANA_0.jpg”) to its identity (for example: “FANA”)\n",
    "ccr_label_fn = lambda img: img.split(\"_\")[0]\n",
    "\n",
    "ccr_model, ccr_class_names = load_model(ccr_models_fc_path, ccr_train_dataset, device, False)\n",
    "ccr_accuracy = evaluate_model(ccr_class_names, ccr_test_dataset, ccr_model, ccr_label_fn, False)\n",
    "print(f\"CCR Accuracy: {ccr_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
