{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/CCR/videos\"\n",
    "annotations_path = \"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/CCR/metadata/annotations/body_data.csv\"\n",
    "dataset_path = \"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/Chimpanzee_detection_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year   video  frame  track         x         y         w         h  \\\n",
      "1316517  2013  13.mp4      1      0  0.680208  0.072917  0.231250  0.450000   \n",
      "1316518  2013  13.mp4      1      2  0.405208  0.137500  0.210417  0.568519   \n",
      "1316519  2013  13.mp4      1      1  0.216667  0.042708  0.255208  0.601852   \n",
      "1316520  2013  13.mp4      2      0  0.679167  0.067708  0.230208  0.457407   \n",
      "1316521  2013  13.mp4      2      2  0.404167  0.137500  0.209375  0.559259   \n",
      "...       ...     ...    ...    ...       ...       ...       ...       ...   \n",
      "1937580  2013  10.mp4  11170    649  0.566667  0.182292  0.182292  0.294444   \n",
      "1937581  2013  10.mp4  11171    649  0.560417  0.180208  0.186458  0.298148   \n",
      "1937582  2013  10.mp4  11171    579  0.340625  0.121875  0.225000  0.435185   \n",
      "1937583  2013  10.mp4  11172    579  0.337500  0.120833  0.226042  0.440741   \n",
      "1937584  2013  10.mp4  11172    649  0.557292  0.167708  0.192708  0.320370   \n",
      "\n",
      "           label  \n",
      "1316517  NOTSURE  \n",
      "1316518    FANWA  \n",
      "1316519   SHARED  \n",
      "1316520  NOTSURE  \n",
      "1316521    FANWA  \n",
      "...          ...  \n",
      "1937580   SHARED  \n",
      "1937581   SHARED  \n",
      "1937582   SHARED  \n",
      "1937583   SHARED  \n",
      "1937584   SHARED  \n",
      "\n",
      "[621068 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(annotations_path)\n",
    "print(df.loc[df[\"year\"] == 2013])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_kept = 0.0015 # approximately 1500 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input: n: size of the sample\n",
    "# proportion: (proportion*100)% of the numbers in [0, n-1]\n",
    "# @outputs\n",
    "# No numbers in common between the outputs\n",
    "def segment_dataset(n, proportion):\n",
    "    # Calculate the number of elements to select based on the proportion\n",
    "    num_elements = int(n * proportion)\n",
    "    \n",
    "    # Generate evenly spaced numbers within the range [0, n-1]\n",
    "    step = n / num_elements\n",
    "    selected_numbers = [int(i * step) for i in range(num_elements)]\n",
    "    \n",
    "    return selected_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_video(path_to_video):\n",
    "    video = cv2.VideoCapture(f\"{video_path}/{path_to_video}\")\n",
    "\n",
    "    if not video.isOpened():\n",
    "        print(f\"Could not open video: {path_to_video}\")\n",
    "        return 0\n",
    "    \n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video.release()\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(video_type, video_id, frame_number):\n",
    "    sample_id = video_id.replace(\".mp4\", \"\")\n",
    "    sub_df = df.loc[(df[\"video\"] == video_id) & (df[\"frame\"] == frame_number)]\n",
    "    output_file = f\"{dataset_path}/labels/{video_type}/{sample_id}_{frame_number}.txt\"\n",
    "\n",
    "    string_output = \"\"\n",
    "\n",
    "    for index, row in sub_df.iterrows():\n",
    "        string_output += f\"\\n0 {row['x']} {row['y']} {row['w']} {row['h']}\"\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(string_output.strip())\n",
    "    file.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# type: \"train\", \"val\" or \"test\" --> indicates which part of the dataset it is\n",
    "# sample_id: name of the video with the extension\n",
    "# idxs: indexes of the images to keep\n",
    "# @output:\n",
    "# nothing, the image are saved in the output folder\n",
    "def extract_data(type, video_id):\n",
    "\n",
    "    sample_id = video_id[:-4]\n",
    "\n",
    "    output_folder = f\"{dataset_path}/images/{type}\"\n",
    "    video = cv2.VideoCapture(f\"{video_path}/{video_id}\")\n",
    "    len_video = get_len_video(video_id)\n",
    "\n",
    "    idxs = segment_dataset(len_video, proportion_kept)\n",
    "    idxs = sorted(idxs)\n",
    "\n",
    "    for frame_count in idxs:\n",
    "\n",
    "        # extract the annotations\n",
    "        extract_metadata(type, video_id, frame_count)\n",
    "\n",
    "        # Set the video to the specific frame\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "        success, frame = video.read()\n",
    "        \n",
    "        if success:\n",
    "            frame_filename = f\"{output_folder}/{sample_id}_{frame_count}.jpg\"\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        else:\n",
    "            print(f\"Could not read frame {frame_count} in video {video_id}\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(video_path)\n",
    "n_files = len(filenames)\n",
    "\n",
    "train_prop = 0.85\n",
    "\n",
    "numbers = list(range(n_files))\n",
    "random.shuffle(numbers)\n",
    "\n",
    "train_video_idxs = numbers[:math.ceil(train_prop*n_files)]\n",
    "val_video_idxs = numbers[math.ceil(train_prop*n_files):]\n",
    "\n",
    "def process_dataset(video_idxs, type):\n",
    "    # progression bar added\n",
    "    for video_idx in tqdm(video_idxs, desc=f\"Processing videos: {type}\", colour=\"green\"):\n",
    "        video_id = filenames[video_idx]\n",
    "        extract_data(type, video_id)\n",
    "        # print(df.loc[df[\"video\"] == sample_id])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: train: 100%|\u001b[32m██████████\u001b[0m| 23/23 [05:36<00:00, 14.65s/it]\n",
      "Processing videos: val: 100%|\u001b[32m██████████\u001b[0m| 4/4 [00:14<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "process_dataset(train_video_idxs, \"train\")\n",
    "process_dataset(val_video_idxs, \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
