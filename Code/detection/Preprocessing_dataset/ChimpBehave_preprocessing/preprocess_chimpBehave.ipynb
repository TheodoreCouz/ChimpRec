{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_path = \"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/chimpbehave/bboxes\"\n",
    "videos_path = \"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/chimpbehave/original\"\n",
    "\n",
    "dataset_path = \"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/Chimpanzee_detection_dataset\" # output dataset\n",
    "\n",
    "video_dim = (1080, 1920) # height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input: n: size of the sample\n",
    "# proportion: (proportion*100)% of the numbers in [0, n-1]\n",
    "# @outputs\n",
    "# No numbers in common between the outputs\n",
    "def segment_dataset(n, proportion):\n",
    "    numbers = list(range(n))\n",
    "    random.shuffle(numbers)\n",
    "\n",
    "    return numbers[:math.ceil(n*proportion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# path: path of the json file to be read\n",
    "# @output:\n",
    "# frame_annotations: a list of lists: [[class x1 y1 x2 y2 confidence], ...] corresponding to the frames of the video\n",
    "def read_json(path):\n",
    "\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except: \n",
    "        print(f\"file: <{path}> not found\")\n",
    "        return \"none\"\n",
    "\n",
    "    frame_annotations = []\n",
    "\n",
    "    for block in data:\n",
    "        annotations = block[\"track_bboxes\"][0][0]\n",
    "        frame_annotations.append(annotations)\n",
    "\n",
    "    return frame_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# type: \"train\", \"val\" or \"test\" --> indicates which part of the dataset it is\n",
    "# sample_id: name of the video without the extension\n",
    "# idxs: indexes of the images to keep\n",
    "# @output:\n",
    "# nothing, the image are saved in the output folder\n",
    "def extract_images_from_video(type, sample_id, idxs):\n",
    "\n",
    "    output_folder = f\"{dataset_path}/images/{type}\"\n",
    "    video = cv2.VideoCapture(f\"{videos_path}/{sample_id}.mp4\")\n",
    "\n",
    "    idxs = sorted(idxs)\n",
    "\n",
    "    for frame_count in idxs:\n",
    "        # Set the video to the specific frame\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "\n",
    "        success, frame = video.read()\n",
    "        \n",
    "        # If frame was successfully read, save it\n",
    "        if success:\n",
    "            frame_filename = f\"{output_folder}/{sample_id}_{frame_count}.jpg\"\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        else:\n",
    "            print(f\"Could not read frame {frame_count} in video {sample_id}\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# x1: top    left  corner of the box (x coordinate)\n",
    "# y1: top    left  corner of the box (y coordinate)\n",
    "# x2: bottom right corner of the box (x coordinate)\n",
    "# y2: bottom right corner of the box (y coordinate)\n",
    "# @outputs:\n",
    "# cx: relative coordinate of the center of the image (x coordinate) \n",
    "# cy: relative coordinate of the center of the image (y coordinate) \n",
    "# h : height of the bbox\n",
    "# w : width of the bbox\n",
    "def convert_coordinates_to_yolo_format(x1, y1, x2, y2):\n",
    "    H, W = video_dim\n",
    "\n",
    "    cx = ((x1 + x2) / 2) / W\n",
    "    cy = ((y1 + y2) / 2) / H\n",
    "\n",
    "    h = (y2 - y1) / H\n",
    "    w = (x2 - x1) / W\n",
    "    \n",
    "    return cx, cy, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inputs:\n",
    "# type: \"train\", \"val\" or \"test\" --> indicates which part of the dataset it is\n",
    "# annotations: data extracted from the json file\n",
    "# filename: name of the file to be saved\n",
    "# idxs: indexes of the images to keep\n",
    "# img_dim: dimensions of the images in the video\n",
    "def extract_data(type, annotations, filename, idxs):\n",
    "    output_folder = f\"{dataset_path}/labels/{type}\"\n",
    "    for idx in idxs:\n",
    "        clss, x1, y1, x2, y2, confidence = annotations[idx]\n",
    "        cx, cy, w, h = convert_coordinates_to_yolo_format(x1, y1, x2, y2)\n",
    "        with open(f\"{output_folder}/{filename}_{idx}.txt\", 'w') as file:\n",
    "            file.write(f\"{clss} {cx} {cy} {w} {h}\")\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/chimpbehave/labels.csv\"\n",
    "labels = pd.read_csv(csv_path)\n",
    "\n",
    "# in the labels.csv file, the action performed by the chimpanzee is indicated. Here is the correspondence:\n",
    "# 0 : sitting\n",
    "# 1 : standing\n",
    "# 2 : walking\n",
    "# 3 : hanging\n",
    "# 4 : climbing_up\n",
    "# 5 : climbing_down \n",
    "# 6 : running\n",
    "# some actions are more static than others. \n",
    "# Static actions provide more redundant information in videos\n",
    "# This is the reason why we want to keep a diffenrent proportion\n",
    "# according to the actions performed by the chimps.\n",
    "# These proportions are defined below:\n",
    "\n",
    "prop_wrt_action = {\n",
    "    0:         0.001, # 0.1%\n",
    "    1:         0.005, # 0.5%\n",
    "    2:         0.4,  # 40%\n",
    "    3:         0.6,  # 60%\n",
    "    4:         1,  # 100%\n",
    "    5:         1,   # 100%\n",
    "    6:         1,   # 100%\n",
    "    \"default\": 0.1    # 10%\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"/home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/chimpbehave/bboxes\")\n",
    "n_files = len(filenames)\n",
    "\n",
    "proportion_kept = 0.8\n",
    "\n",
    "train_prop = 0.85*proportion_kept\n",
    "val_prop = (1-train_prop)*proportion_kept\n",
    "\n",
    "numbers = list(range(n_files))\n",
    "random.shuffle(numbers)\n",
    "\n",
    "train_video_idxs = numbers[:math.ceil(train_prop*n_files)]\n",
    "val_video_idxs = numbers[math.ceil(train_prop*n_files):]\n",
    "\n",
    "def process_dataset(video_idxs, type):\n",
    "    # progression bar added\n",
    "    for video_idx in tqdm(video_idxs, desc=f\"Processing videos: {type}\", colour=\"green\"):\n",
    "        filename = filenames[video_idx]\n",
    "        sample_id = filename.strip(\"_bboxes.json\")\n",
    "\n",
    "        # type of movement performed by the chimp\n",
    "        try: class_id = int(labels.loc[labels[\"new_filename_with_ext\"] == f\"{sample_id}.mp4\"][\"class_id\"])\n",
    "        except: \n",
    "            class_id = \"default\" # the correspondence between the file names is wrong, default class\n",
    "            print(\"default class\")\n",
    "\n",
    "        proportion = prop_wrt_action[class_id]\n",
    "\n",
    "        try :\n",
    "            json_path = f'{bboxes_path}/{sample_id}_bboxes.json'\n",
    "            annotations = read_json(json_path)\n",
    "\n",
    "            idxs = segment_dataset(len(annotations), proportion*0.1)\n",
    "\n",
    "            extract_data(type, annotations, sample_id, idxs)\n",
    "            extract_images_from_video(type, sample_id, idxs)\n",
    "        except:\n",
    "            print(f\"sample <{sample_id}> couldn't be treated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: train:   0%|\u001b[32m          \u001b[0m| 0/869 [00:00<?, ?it/s]/tmp/ipykernel_11459/4228228612.py:20: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  try: class_id = int(labels.loc[labels[\"new_filename_with_ext\"] == f\"{sample_id}.mp4\"][\"class_id\"])\n",
      "Processing videos: train:  32%|\u001b[32m███▏      \u001b[0m| 279/869 [00:40<01:46,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: train:  67%|\u001b[32m██████▋   \u001b[0m| 578/869 [01:23<00:31,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: train:  74%|\u001b[32m███████▎  \u001b[0m| 639/869 [01:34<00:23,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default class\n",
      "file: </home/theo/Documents/Unif/Master/ChimpRec/ChimpRec-Dataset/chimpbehave/bboxes/ipynb_checkpoint_bboxes.json> not found\n",
      "sample <ipynb_checkpoint> couldn't be treated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: train: 100%|\u001b[32m██████████\u001b[0m| 869/869 [02:09<00:00,  6.73it/s]\n",
      "Processing videos: val: 100%|\u001b[32m██████████\u001b[0m| 153/153 [00:19<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "process_dataset(train_video_idxs, \"train\")\n",
    "process_dataset(val_video_idxs, \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
