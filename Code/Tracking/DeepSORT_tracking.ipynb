{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchreid\\reid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort.deep_sort.tracker import Tracker as DeepSortTracker\n",
    "from deep_sort.tools import generate_detections as gdet\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "import torchreid\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Tracker:\\n    tracker = None\\n    encoder = None\\n    tracks = None\\n\\n    def __init__(self):\\n        max_cosine_distance = 0.4\\n        nn_budget = None\\n\\n        encoder_model_filename = \\'model_data/mars-small128.pb\\'\\n\\n        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\\n        self.tracker = DeepSortTracker(metric)\\n        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)\\n\\n    def update(self, frame, detections):\\n\\n        if len(detections) == 0:\\n            self.tracker.predict()\\n            self.tracker.update([])  \\n            self.update_tracks()\\n            return\\n\\n        bboxes = np.asarray([d[:-1] for d in detections])\\n        bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\\n        scores = [d[-1] for d in detections]\\n\\n        features = self.encoder(frame, bboxes)\\n\\n        dets = []\\n        for bbox_id, bbox in enumerate(bboxes):\\n            dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\\n\\n        self.tracker.predict()\\n        self.tracker.update(dets)\\n        self.update_tracks()\\n\\n    def update_tracks(self):\\n        tracks = []\\n        for track in self.tracker.tracks:\\n            if not track.is_confirmed() or track.time_since_update > 1:\\n                continue\\n            bbox = track.to_tlbr()\\n\\n            id = track.track_id\\n\\n            tracks.append(Track(id, bbox))\\n\\n        self.tracks = tracks\\n\\n\\nclass Track:\\n    track_id = None\\n    bbox = None\\n\\n    def __init__(self, id, bbox):\\n        self.track_id = id\\n        self.bbox = bbox'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class Tracker:\n",
    "    tracker = None\n",
    "    encoder = None\n",
    "    tracks = None\n",
    "\n",
    "    def __init__(self):\n",
    "        max_cosine_distance = 0.4\n",
    "        nn_budget = None\n",
    "\n",
    "        encoder_model_filename = 'model_data/mars-small128.pb'\n",
    "\n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "        self.tracker = DeepSortTracker(metric)\n",
    "        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)\n",
    "\n",
    "    def update(self, frame, detections):\n",
    "\n",
    "        if len(detections) == 0:\n",
    "            self.tracker.predict()\n",
    "            self.tracker.update([])  \n",
    "            self.update_tracks()\n",
    "            return\n",
    "\n",
    "        bboxes = np.asarray([d[:-1] for d in detections])\n",
    "        bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\n",
    "        scores = [d[-1] for d in detections]\n",
    "\n",
    "        features = self.encoder(frame, bboxes)\n",
    "\n",
    "        dets = []\n",
    "        for bbox_id, bbox in enumerate(bboxes):\n",
    "            dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\n",
    "\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(dets)\n",
    "        self.update_tracks()\n",
    "\n",
    "    def update_tracks(self):\n",
    "        tracks = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()\n",
    "\n",
    "            id = track.track_id\n",
    "\n",
    "            tracks.append(Track(id, bbox))\n",
    "\n",
    "        self.tracks = tracks\n",
    "\n",
    "\n",
    "class Track:\n",
    "    track_id = None\n",
    "    bbox = None\n",
    "\n",
    "    def __init__(self, id, bbox):\n",
    "        self.track_id = id\n",
    "        self.bbox = bbox\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"C:/Users/julie/OneDrive - UCL/Master_2/Mémoire/ChimpRec/Code/Body_detection/YOLO_small/runs/detect/train9/weights/best.pt\")\n",
    "video_path = \"C:/Users/julie/Documents/Unif/Mémoire/tracking_video_test2.mp4\" #Vidéo input\n",
    "output_path = \"tracking_test_output7.mp4\" \n",
    "file_improve_tracking_path = \"C:/Users/julie/Documents/Unif/Mémoire/tracking_improvement.txt\" #Fichier dans lequel les annotations de bbox sont écrites \n",
    "\n",
    "# Paramètres de DeepSORT\n",
    "max_cosine_distance = 0.3  # Distance max pour matcher un objet (plus bas = plus strict)\n",
    "nn_budget = None  # Taille max du buffer pour le modèle d'appariement\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "\n",
    "# Initialiser DeepSORT\n",
    "tracker = DeepSortTracker(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(image, color, bbox, label):\n",
    "    x1, y1, x2, y2, score = bbox\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    factor = 0.65 if label == \"Face\" else 0.3\n",
    "    font_scale = max(0.4, (x2 - x1 + y2 - y1) / 300) * factor\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 4)\n",
    "    label_text = f\"{label}\"#: {score:.2f}\"\n",
    "    (w, h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_COMPLEX, font_scale, 1)\n",
    "\n",
    "    overlay = image.copy()\n",
    "    cv2.rectangle(overlay, (x2 - w - 10, y2 - h - 10), (x2, y2), color, -1)\n",
    "    cv2.addWeighted(overlay, 0.5, image, 0.5, 0, image)\n",
    "\n",
    "    cv2.putText(image, label_text, (x2 - w - 5, y2 - 5), cv2.FONT_HERSHEY_COMPLEX, font_scale, (255,255,255), 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"C:\\Users\\julie/.cache\\torch\\checkpoints\\osnet_x1_0_imagenet.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "# Charger OSNet pré-entraîné sur Market-1501\n",
    "model_feature_extraction = torchreid.models.build_model(name='osnet_x1_0', num_classes=751, pretrained=True)\n",
    "model_feature_extraction.eval()  # Mode évaluation\n",
    "\n",
    "# Transformer l'image pour OSNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),  # Taille attendue par OSNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_features_osnet(frame, bbox):\n",
    "    \"\"\"Extrait les features d'un chimpanzé avec OSNet.\"\"\"\n",
    "    x1, y1, x2, y2 = map(int, bbox)  # Convertir bbox en entiers\n",
    "    chimp_img = frame[y1:y2, x1:x2]  # Extraire la région d'intérêt\n",
    "\n",
    "    chimp_img = cv2.cvtColor(chimp_img, cv2.COLOR_BGR2RGB)  # Convertir en RGB\n",
    "    chimp_img = transform(Image.fromarray(chimp_img)).unsqueeze(0)  # Appliquer transformations\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature = model_feature_extraction(chimp_img)  # Extraire les features\n",
    "\n",
    "    # Convertir les features en vecteur 1D et s'assurer qu'ils ont une forme correcte\n",
    "    feature = feature.cpu().numpy().flatten()  # Retourner un vecteur 512D\n",
    "\n",
    "    return feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(colors, colors_used, video_path, file_improve_tracking_path, output_path, model, tracker): \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    file_improve_tracking = open(file_improve_tracking_path, \"w\")\n",
    "    while ret: \n",
    "        file_improve_tracking.write(\"#\\n\")\n",
    "        predictions = model.predict(frame, verbose=False)[0]\n",
    "        \n",
    "        #On ne garde que les détections de bboxes qui ont une confiance suppérieur à 0.2\n",
    "        detections = []\n",
    "        for x1, y1, x2, y2, score, _ in predictions.boxes.data.tolist():\n",
    "            if score >= 0.2:\n",
    "                bbox = np.array([x1, y1, x2 - x1, y2 - y1]) #Coin supérieur gauche + width et height \n",
    "                confidence = float(score)\n",
    "                #feature permet à DeepSORT de prendre en compte l'apparence en plus dans le tracking donc il faudrait extraire l'apparence grâce à un model \n",
    "                #Fonctionne un peu comme SORT pour l'instant ducoup car l'apparence est pas prise en compte \n",
    "                feature = extract_features_osnet(frame, [x1, y1, x2, y2]) \n",
    "                detections.append(Detection(bbox, confidence, feature))\n",
    "\n",
    "        # Mettre à jour avec les détections et forcer l'association des features\n",
    "        if len(detections) > 0:\n",
    "            tracker.predict()  # Prédit la position des objets dans la frame suivante\n",
    "            tracker.update(detections)  # Maj avec les détections\n",
    "\n",
    "\n",
    "        #Dessiner les bboxes\n",
    "        for track in tracker.tracks: #Parcourt les objets suivis\n",
    "            if track.is_confirmed() and track.track_id not in tracker.metric.samples:\n",
    "                tracker.metric.samples[track.track_id] = []\n",
    "\n",
    "            #Si pour un objet suivi il y a des doutes sur son identité ou si pas été mis à jour depuis trop de temps, on l'ignore.\n",
    "            if not track.is_confirmed() or track.time_since_update > 1: \n",
    "                if track.track_id in colors_used: \n",
    "                    colors.append(colors_used[track.track_id])\n",
    "                    colors_used.pop(track.track_id)\n",
    "                continue\n",
    "\n",
    "            bbox = track.to_tlbr()  # Format [x1, y1, x2, y2]\n",
    "            track_id = track.track_id\n",
    "            class_id = \"Chimp\"\n",
    "            if track_id not in colors_used.keys(): \n",
    "                colors_used[track_id] = colors.pop(0) \n",
    "            color = colors_used[track_id]\n",
    "\n",
    "            # Dessiner la bounding box avec l'ID du chimpanzé\n",
    "            str_bbox = ' '.join(map(str, bbox))\n",
    "            file_improve_tracking.write(f\"{track_id} {str_bbox}\\n\")\n",
    "            draw_bbox(frame, color, (*bbox, 1), f\"{class_id} {track_id}\")\n",
    "\n",
    "        # Afficher la vidéo avec le suivi\n",
    "        out.write(frame)\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "    file_improve_tracking.close()\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m\n\u001b[0;32m      1\u001b[0m colors_used \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m99\u001b[39m),\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     (\u001b[38;5;241m218\u001b[39m, \u001b[38;5;241m165\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     44\u001b[0m ]\n\u001b[1;32m---> 46\u001b[0m \u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors_used\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_improve_tracking_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m, in \u001b[0;36mtrack\u001b[1;34m(colors, colors_used, video_path, file_improve_tracking_path, output_path, model, tracker)\u001b[0m\n\u001b[0;32m     19\u001b[0m         confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(score)\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m#feature permet à DeepSORT de prendre en compte l'apparence en plus dans le tracking donc il faudrait extraire l'apparence grâce à un model \u001b[39;00m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m#Fonctionne un peu comme SORT pour l'instant ducoup car l'apparence est pas prise en compte \u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m         feature \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_osnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     23\u001b[0m         detections\u001b[38;5;241m.\u001b[39mappend(Detection(bbox, confidence, feature))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Mettre à jour avec les détections et forcer l'association des features\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mextract_features_osnet\u001b[1;34m(frame, bbox)\u001b[0m\n\u001b[0;32m     15\u001b[0m chimp_img \u001b[38;5;241m=\u001b[39m frame[y1:y2, x1:x2]  \u001b[38;5;66;03m# Extraire la région d'intérêt\u001b[39;00m\n\u001b[0;32m     17\u001b[0m chimp_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(chimp_img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)  \u001b[38;5;66;03m# Convertir en RGB\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m chimp_img \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchimp_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Appliquer transformations\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     21\u001b[0m     feature \u001b[38;5;241m=\u001b[39m model_feature_extraction(chimp_img)  \u001b[38;5;66;03m# Extraire les features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:349\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:926\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    925\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "colors_used = {}\n",
    "\n",
    "colors = [\n",
    "    (120, 50, 99),\n",
    "    (180, 25, 16),\n",
    "    (73, 89, 176),\n",
    "    (200, 158, 18),\n",
    "    (199, 214, 152),\n",
    "    (181, 37, 229),\n",
    "    (118, 73, 165),\n",
    "    (136, 3, 53),\n",
    "    (40, 47, 142),\n",
    "    (246, 26, 168),\n",
    "    (33, 83, 190),\n",
    "    (151, 220, 243),\n",
    "    (156, 122, 217),\n",
    "    (173, 0, 128),\n",
    "    (61, 242, 230),\n",
    "    (37, 10, 125),\n",
    "    (64, 229, 201),\n",
    "    (64, 137, 49),\n",
    "    (136, 225, 85),\n",
    "    (146, 80, 77),\n",
    "    (255, 0, 0),\n",
    "    (0, 255, 0),\n",
    "    (0, 0, 255),\n",
    "    (255, 255, 0),\n",
    "    (0, 255, 255),\n",
    "    (255, 0, 255),\n",
    "    (255, 165, 0),\n",
    "    (255, 255, 255),\n",
    "    (0, 0, 0),\n",
    "    (128, 0, 0),\n",
    "    (0, 128, 0),\n",
    "    (128, 128, 0),\n",
    "    (0, 128, 128),\n",
    "    (128, 0, 128),\n",
    "    (255, 105, 180),\n",
    "    (255, 69, 0),\n",
    "    (34, 139, 34),\n",
    "    (70, 130, 180),\n",
    "    (255, 228, 225),\n",
    "    (218, 165, 32)\n",
    "]\n",
    "\n",
    "track(colors, colors_used, video_path, file_improve_tracking_path, output_path, model, tracker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
