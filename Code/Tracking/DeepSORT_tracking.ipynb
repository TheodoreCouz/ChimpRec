{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deep_sort.deep_sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tracker \u001b[38;5;28;01mas\u001b[39;00m DeepSortTracker\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_detections \u001b[38;5;28;01mas\u001b[39;00m gdet\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn_matching\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deep_sort.deep_sort'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/Theo/Documents/Unif\")\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort.deep_sort.tracker import Tracker as DeepSortTracker\n",
    "from deep_sort.tools import generate_detections as gdet\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "import torchreid\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Tracker:\\n    tracker = None\\n    encoder = None\\n    tracks = None\\n\\n    def __init__(self):\\n        max_cosine_distance = 0.4\\n        nn_budget = None\\n\\n        encoder_model_filename = \\'model_data/mars-small128.pb\\'\\n\\n        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\\n        self.tracker = DeepSortTracker(metric)\\n        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)\\n\\n    def update(self, frame, detections):\\n\\n        if len(detections) == 0:\\n            self.tracker.predict()\\n            self.tracker.update([])  \\n            self.update_tracks()\\n            return\\n\\n        bboxes = np.asarray([d[:-1] for d in detections])\\n        bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\\n        scores = [d[-1] for d in detections]\\n\\n        features = self.encoder(frame, bboxes)\\n\\n        dets = []\\n        for bbox_id, bbox in enumerate(bboxes):\\n            dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\\n\\n        self.tracker.predict()\\n        self.tracker.update(dets)\\n        self.update_tracks()\\n\\n    def update_tracks(self):\\n        tracks = []\\n        for track in self.tracker.tracks:\\n            if not track.is_confirmed() or track.time_since_update > 1:\\n                continue\\n            bbox = track.to_tlbr()\\n\\n            id = track.track_id\\n\\n            tracks.append(Track(id, bbox))\\n\\n        self.tracks = tracks\\n\\n\\nclass Track:\\n    track_id = None\\n    bbox = None\\n\\n    def __init__(self, id, bbox):\\n        self.track_id = id\\n        self.bbox = bbox'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class Tracker:\n",
    "    tracker = None\n",
    "    encoder = None\n",
    "    tracks = None\n",
    "\n",
    "    def __init__(self):\n",
    "        max_cosine_distance = 0.4\n",
    "        nn_budget = None\n",
    "\n",
    "        encoder_model_filename = 'model_data/mars-small128.pb'\n",
    "\n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "        self.tracker = DeepSortTracker(metric)\n",
    "        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)\n",
    "\n",
    "    def update(self, frame, detections):\n",
    "\n",
    "        if len(detections) == 0:\n",
    "            self.tracker.predict()\n",
    "            self.tracker.update([])  \n",
    "            self.update_tracks()\n",
    "            return\n",
    "\n",
    "        bboxes = np.asarray([d[:-1] for d in detections])\n",
    "        bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\n",
    "        scores = [d[-1] for d in detections]\n",
    "\n",
    "        features = self.encoder(frame, bboxes)\n",
    "\n",
    "        dets = []\n",
    "        for bbox_id, bbox in enumerate(bboxes):\n",
    "            dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\n",
    "\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(dets)\n",
    "        self.update_tracks()\n",
    "\n",
    "    def update_tracks(self):\n",
    "        tracks = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()\n",
    "\n",
    "            id = track.track_id\n",
    "\n",
    "            tracks.append(Track(id, bbox))\n",
    "\n",
    "        self.tracks = tracks\n",
    "\n",
    "\n",
    "class Track:\n",
    "    track_id = None\n",
    "    bbox = None\n",
    "\n",
    "    def __init__(self, id, bbox):\n",
    "        self.track_id = id\n",
    "        self.bbox = bbox\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"C:/Users/Theo/Documents/Unif/Models/body/v8s/weights/best.pt\")\n",
    "video_path = \"C:/Users/Theo/Documents/Unif/chimprec-videos/sample1.MP4\" #Vidéo input\n",
    "output_path = \"C:/Users/Theo/Documents/Unif/chimprec-videos/output1.MP4\"\n",
    "file_improve_tracking_path = \"C:/Users/Theo/Documents/Unif/chimprec-videos/metadata.txt\" #Fichier dans lequel les annotations de bbox sont écrites \n",
    "\n",
    "# Paramètres de DeepSORT\n",
    "max_cosine_distance = 0.5  # Distance max pour matcher un objet (plus bas = plus strict)\n",
    "nn_budget = None  # Taille max du buffer pour le modèle d'appariement\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "\n",
    "# Initialiser DeepSORT\n",
    "tracker = DeepSortTracker(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(image, color, bbox, label):\n",
    "    x1, y1, x2, y2, score = bbox\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    factor = 0.65 if label == \"Face\" else 0.3\n",
    "    font_scale = max(0.4, (x2 - x1 + y2 - y1) / 300) * factor\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 4)\n",
    "    label_text = f\"{label}\"#: {score:.2f}\"\n",
    "    (w, h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_COMPLEX, font_scale, 1)\n",
    "\n",
    "    overlay = image.copy()\n",
    "    cv2.rectangle(overlay, (x2 - w - 10, y2 - h - 10), (x2, y2), color, -1)\n",
    "    cv2.addWeighted(overlay, 0.5, image, 0.5, 0, image)\n",
    "\n",
    "    cv2.putText(image, label_text, (x2 - w - 5, y2 - 5), cv2.FONT_HERSHEY_COMPLEX, font_scale, (255,255,255), 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"C:\\Users\\julie/.cache\\torch\\checkpoints\\osnet_x1_0_imagenet.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "# Charger OSNet pré-entraîné sur Market-1501\n",
    "model_feature_extraction = torchreid.models.build_model(name='osnet_x1_0', num_classes=751, pretrained=True)\n",
    "model_feature_extraction.eval()  # Mode évaluation\n",
    "\n",
    "# Transformer l'image pour OSNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),  # Taille attendue par OSNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_features_osnet(frame, bbox):\n",
    "    \"\"\"Extrait les features d'un chimpanzé avec OSNet.\"\"\"\n",
    "    x1, y1, x2, y2 = map(int, bbox)  # Convertir bbox en entiers\n",
    "    chimp_img = frame[y1:y2, x1:x2]  # Extraire la région d'intérêt\n",
    "\n",
    "    chimp_img = cv2.cvtColor(chimp_img, cv2.COLOR_BGR2RGB)  # Convertir en RGB\n",
    "    chimp_img = transform(Image.fromarray(chimp_img)).unsqueeze(0)  # Appliquer transformations\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature = model_feature_extraction(chimp_img)  # Extraire les features\n",
    "\n",
    "    # Convertir les features en vecteur 1D et s'assurer qu'ils ont une forme correcte\n",
    "    feature = feature.cpu().numpy().flatten()  # Retourner un vecteur 512D\n",
    "\n",
    "    return feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(colors, colors_used, video_path, file_improve_tracking_path, output_path, model, tracker): \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    file_improve_tracking = open(file_improve_tracking_path, \"w\")\n",
    "    while ret: \n",
    "        file_improve_tracking.write(\"#\\n\")\n",
    "        predictions = model.predict(frame, verbose=False)[0]\n",
    "        \n",
    "        #On ne garde que les détections de bboxes qui ont une confiance suppérieur à 0.2\n",
    "        detections = []\n",
    "        for x1, y1, x2, y2, score, _ in predictions.boxes.data.tolist():\n",
    "            if score >= 0.2:\n",
    "                bbox = np.array([x1, y1, x2 - x1, y2 - y1]) #Coin supérieur gauche + width et height \n",
    "                confidence = float(score)\n",
    "                #feature permet à DeepSORT de prendre en compte l'apparence en plus dans le tracking donc il faudrait extraire l'apparence grâce à un model \n",
    "                #Fonctionne un peu comme SORT pour l'instant ducoup car l'apparence est pas prise en compte \n",
    "                feature = extract_features_osnet(frame, [x1, y1, x2, y2]) \n",
    "                detections.append(Detection(bbox, confidence, feature))\n",
    "\n",
    "        # Mettre à jour avec les détections et forcer l'association des features\n",
    "        if len(detections) > 0:\n",
    "            tracker.predict()  # Prédit la position des objets dans la frame suivante\n",
    "            tracker.update(detections)  # Maj avec les détections\n",
    "\n",
    "\n",
    "        #Dessiner les bboxes\n",
    "        for track in tracker.tracks: #Parcourt les objets suivis\n",
    "            if track.is_confirmed() and track.track_id not in tracker.metric.samples:\n",
    "                tracker.metric.samples[track.track_id] = []\n",
    "\n",
    "            #Si pour un objet suivi il y a des doutes sur son identité ou si pas été mis à jour depuis trop de temps, on l'ignore.\n",
    "            if not track.is_confirmed() or track.time_since_update > 1: \n",
    "                if track.track_id in colors_used: \n",
    "                    colors.append(colors_used[track.track_id])\n",
    "                    colors_used.pop(track.track_id)\n",
    "                continue\n",
    "\n",
    "            bbox = track.to_tlbr()  # Format [x1, y1, x2, y2]\n",
    "            track_id = track.track_id\n",
    "            class_id = \"Chimp\"\n",
    "            if track_id not in colors_used.keys(): \n",
    "                colors_used[track_id] = colors.pop(0) \n",
    "            color = colors_used[track_id]\n",
    "\n",
    "            # Dessiner la bounding box avec l'ID du chimpanzé\n",
    "            str_bbox = ' '.join(map(str, bbox))\n",
    "            file_improve_tracking.write(f\"{track_id} {str_bbox}\\n\")\n",
    "            draw_bbox(frame, color, (*bbox, 1), f\"{class_id} {track_id}\")\n",
    "\n",
    "        # Afficher la vidéo avec le suivi\n",
    "        out.write(frame)\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "    file_improve_tracking.close()\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_used = {}\n",
    "\n",
    "colors = [\n",
    "    (120, 50, 99),\n",
    "    (180, 25, 16),\n",
    "    (73, 89, 176),\n",
    "    (200, 158, 18),\n",
    "    (199, 214, 152),\n",
    "    (181, 37, 229),\n",
    "    (118, 73, 165),\n",
    "    (136, 3, 53),\n",
    "    (40, 47, 142),\n",
    "    (246, 26, 168),\n",
    "    (33, 83, 190),\n",
    "    (151, 220, 243),\n",
    "    (156, 122, 217),\n",
    "    (173, 0, 128),\n",
    "    (61, 242, 230),\n",
    "    (37, 10, 125),\n",
    "    (64, 229, 201),\n",
    "    (64, 137, 49),\n",
    "    (136, 225, 85),\n",
    "    (146, 80, 77),\n",
    "    (255, 0, 0),\n",
    "    (0, 255, 0),\n",
    "    (0, 0, 255),\n",
    "    (255, 255, 0),\n",
    "    (0, 255, 255),\n",
    "    (255, 0, 255),\n",
    "    (255, 165, 0),\n",
    "    (255, 255, 255),\n",
    "    (0, 0, 0),\n",
    "    (128, 0, 0),\n",
    "    (0, 128, 0),\n",
    "    (128, 128, 0),\n",
    "    (0, 128, 128),\n",
    "    (128, 0, 128),\n",
    "    (255, 105, 180),\n",
    "    (255, 69, 0),\n",
    "    (34, 139, 34),\n",
    "    (70, 130, 180),\n",
    "    (255, 228, 225),\n",
    "    (218, 165, 32)\n",
    "]\n",
    "\n",
    "track(colors, colors_used, video_path, file_improve_tracking_path, output_path, model, tracker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
