{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>File paths and imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ui_lib import *\n",
    "\n",
    "# path of the model\n",
    "model_path = \"...\"\n",
    "\n",
    "# video paths:\n",
    "# input video (without any annotation)\n",
    "input_video_path = \"C:/Users/Theo/Documents/Unif/chimprec-videos/sample_1/sample1.MP4\"\n",
    "# output (tracking without human-based improvements)\n",
    "output_video_path = \"C:/Users/Theo/Documents/Unif/chimprec-videos/sample_1/sample1_test.MP4\"\n",
    "# output (final version - with human interaction)\n",
    "output_edited_video_path = \"C:/Users/Theo/Documents/Unif/ChimpRec/Code/Tracking/user_interaction/sample_1/output_edited.txt\"\n",
    "\n",
    "# text file paths\n",
    "# text file containing the output of the tracking operations\n",
    "input_text_file_path = \"C:/Users/Theo/Documents/Unif/ChimpRec/Code/Tracking/user_interaction/test_files/raw_output.txt\"\n",
    "# text file containing the manually produce modifications\n",
    "edit_path = \"C:/Users/Theo/Documents/Unif/ChimpRec/Code/Tracking/user_interaction/test_files/edit_stage1.txt\"\n",
    "# final output text file (this is a modification of <input_text_file_path> based on <edit_path>)\n",
    "output_text_file_path = \"C:/Users/Theo/Documents/Unif/ChimpRec/Code/Tracking/user_interaction/test_files/edited_w_swaps.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>First step:</h2>\n",
    "<h3>Processing the raw video without annotation and produce a textual output (stored in <i>input_text_file_path</i>) and a visual output (accessible via <i>output_video_path</i>).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"...\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "max_cosine_distance = 0.5       # maximal distance to match an object (lower = more strict)\n",
    "nn_budget = None                # maximal buffer size\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "\n",
    "# YOLOv8s initialisation\n",
    "YOLOv8s = YOLO(model_path)\n",
    "\n",
    "# DeepSORT initialisation\n",
    "DeepSort = DeepSortTracker(metric)\n",
    "\n",
    "# Osnet initialisation\n",
    "Osnet = torchreid.models.build_model(name='osnet_x1_0', num_classes=751, pretrained=True)\n",
    "Osnet.eval()\n",
    "\n",
    "# production of the textual output\n",
    "perform_tracking(\n",
    "    input_video_path = input_video_path, \n",
    "    output_text_file_path = input_text_file_path, \n",
    "    detection_model = YOLOv8s, \n",
    "    tracker = DeepSort,\n",
    "    confidence_threshold = 0.5, \n",
    "    model_feature_extraction = Osnet\n",
    ")\n",
    "\n",
    "# production of the visual output\n",
    "draw_bbox_from_file(\n",
    "    file_path = input_text_file_path, \n",
    "    input_video_path = input_video_path, \n",
    "    output_video_path = output_video_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# production of the visual output\n",
    "draw_bbox_from_file(\n",
    "    file_path = input_text_file_path, \n",
    "    input_video_path = input_video_path, \n",
    "    output_video_path = output_video_path,\n",
    "    draw_frame_count=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Second step: \n",
    "\n",
    "Human interaction required to bind the chains of detections together and perform recognition</h2>\n",
    "<h3>This part of the process is based on a human edition. The modifications have to:</h3>\n",
    "\n",
    "* Be econded according to a very well defined format (see explanations below)\n",
    "* Be stored in the text file located at <i>edit_path</i>\n",
    "\n",
    "<h3>The edition file aims at solving two major tasks:</h3>\n",
    "\n",
    "* Merging some chains of detections.\n",
    "* Labelising the chimpanzees with their names when they can be identified instead of ids. When no name is given, a default name is written (UNK_X).\n",
    "\n",
    "<h3>Format of the edition file:</h3>\n",
    "\n",
    "* The identifiers of the chains of detections to be merged have to be written on the same line.\n",
    "    * As a result, if a chain of detections needs to appear without being merged, its identifier must be isolated on a line written in the edition file.\n",
    "* If a chain of detections has to be removed, then its identifier must not appear in the edition file. \n",
    "* If a name can be attached to set of chains of detections, then it must be written on the same same line as them separated by \": \" (note: the space character is important)\n",
    "\n",
    "<h3>Concrete examples:</h3>\n",
    "\n",
    "![Example 1](images/ex1.png)\n",
    "![Example 2](images/ex2.png)\n",
    "\n",
    "As we can see on those two images, the same individual belongs to two subsequent chains of detections (respectly 6 and 10). The following line must appear in the edition file if you want to merge them:\n",
    "\n",
    "<b>6 10</b>\n",
    "\n",
    "If you are able to identify the identity of this indiviual (say its name is \"Muke\"), then you can type the following line to attach it a name:\n",
    "\n",
    "<b>Muke: 6 10</b>\n",
    "\n",
    "![Example 3](images/ex3.png)\n",
    "\n",
    "In this example, we are interested in removing the boxes 52 but keep the boxes 43. To do so, you simply need to not mention the id 52 in the edition file. However, 43 has to appear even if it would be alone on a line. For instance, the following line can possibly figure in the edition file:\n",
    "\n",
    "<b>43</b>\n",
    "\n",
    "Finally, if a chimpanzee cannot be recognized, you can still merge the chains of detection and a default name will be assigned.\n",
    "\n",
    "![Example 4](images/ex4.png)\n",
    "\n",
    "This picture shows an example of individuals that couldn't have been recognised (note: UNK is used to designate unkown in shorts).\n",
    "\n",
    "Once this step is completed, all the human processing is finished. Meaning that the execution of the following code is going to produce a correction of the first output video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Third step:</h2>\n",
    "<h3>Processing the output of the two first steps to produce a textual output (stored in <i>output_text_file_path</i>) and a visual output (accessible via <i>output_edited_video_path</i>).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': [('1', '2')], '2': [('1', '1')]}\n",
      "[[['UNK_0', '1', '1', '1', '1'], ['UNK_1', '2', '2', '2', '2']], [['UNK_1', '1', '1', '1', '1'], ['UNK_0', '2', '2', '2', '2']], [['UNK_1', '1', '1', '1', '1'], ['UNK_0', '2', '2', '2', '2']], [['UNK_1', '2', '2', '2', '2']]]\n"
     ]
    }
   ],
   "source": [
    "# instanciating the readers and writer\n",
    "raw_reader = raw_tracking_data_reader(input_text_file_path)\n",
    "edit_reader = modification_reader(edit_path)\n",
    "writer = data_writer(output_text_file_path)\n",
    "\n",
    "modified_data = edit_raw_output(raw_reader, edit_reader)  \n",
    "\n",
    "# production of the textual output\n",
    "writer.write(modified_data)\n",
    "\n",
    "# # production of the visual output\n",
    "# draw_bbox_from_file(\n",
    "#     file_path = output_text_file_path, \n",
    "#     input_video_path = input_video_path, \n",
    "#     output_video_path = output_edited_video_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK_0', ['3']], ['UNK_1', ['4', '2']]]\n"
     ]
    }
   ],
   "source": [
    "# test cell\n",
    "from ui_lib import *\n",
    "raw_path = \"...\"\n",
    "edit_path = \"...\"\n",
    "output_file_path = \"...\"\n",
    "\n",
    "raw_reader = raw_tracking_data_reader(raw_path)\n",
    "edit_reader = modification_reader(edit_path)\n",
    "\n",
    "print(edit_reader.data)\n",
    "\n",
    "writer = data_writer(output_file_path)\n",
    "\n",
    "modified_data = edit_raw_output(raw_reader, edit_reader)  \n",
    "writer.write(modified_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
