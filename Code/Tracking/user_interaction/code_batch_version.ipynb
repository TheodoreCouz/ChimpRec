{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>File paths and imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\torchreid\\reid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ui_lib import *\n",
    "\n",
    "# path of the model\n",
    "model_path = \"C:/Users/Theo/Documents/Unif/Models/body/v8s/weights/best.pt\"\n",
    "\n",
    "# video paths:\n",
    "# input video directory (without any annotation)\n",
    "input_video_directory = \"C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos\"\n",
    "# output (final version - with human interaction)\n",
    "output_video_directory = \"C:/Users/Theo/Documents/Unif/chimprec-videos/output_videos\"\n",
    "# directory containing all the manual modifications\n",
    "mannual_annotations_directory = \"C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos/manual_annotations\"\n",
    "\n",
    "output_video_directory_temp = f\"{output_video_directory}/temp\"\n",
    "raw_text_output_directory = f\"{output_video_directory_temp}/raw_output\"\n",
    "\n",
    "# Create the directories if they do not exist yet\n",
    "os.makedirs(input_video_directory, exist_ok=True)\n",
    "os.makedirs(output_video_directory, exist_ok=True)\n",
    "os.makedirs(mannual_annotations_directory, exist_ok=True)\n",
    "os.makedirs(output_video_directory_temp, exist_ok=True)\n",
    "os.makedirs(raw_text_output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>First step:</h2>\n",
    "<h3>Processing the raw video without annotation and produce a textual output (stored in <i>input_text_file_path</i>) and a visual output (accessible via <i>output_video_path</i>).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"C:\\Users\\Theo/.cache\\torch\\checkpoints\\osnet_x1_0_imagenet.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Annotations ready for video: C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos\\C0005.MP4\n",
      "Treatment done: C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos\\C0005.MP4\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Annotations ready for video: C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos\\sample1.MP4\n",
      "Treatment done: C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos\\sample1.MP4\n"
     ]
    }
   ],
   "source": [
    "max_cosine_distance = 0.5       # maximal distance to match an object (lower = more strict)\n",
    "nn_budget = None                # maximal buffer size\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "\n",
    "# YOLOv8s initialisation\n",
    "YOLOv8s = YOLO(model_path)\n",
    "\n",
    "# DeepSORT initialisation\n",
    "DeepSort = DeepSortTracker(metric)\n",
    "\n",
    "# Osnet initialisation\n",
    "Osnet = torchreid.models.build_model(name='osnet_x1_0', num_classes=751, pretrained=True)\n",
    "Osnet.eval()\n",
    "\n",
    "for input_video in os.listdir(input_video_directory):\n",
    "    if input_video.endswith(\".mp4\") or input_video.endswith(\".MP4\"):\n",
    "\n",
    "        full_video_path = os.path.join(input_video_directory, input_video)\n",
    "        video_name = os.path.splitext(input_video)[0]\n",
    "\n",
    "        # production of the textual outputs\n",
    "        perform_tracking(\n",
    "            input_video_path = full_video_path, \n",
    "            output_text_file_path = f\"{raw_text_output_directory}/{video_name}.txt\", \n",
    "            detection_model = YOLOv8s, \n",
    "            tracker = DeepSort,\n",
    "            confidence_threshold = 0.5, \n",
    "            model_feature_extraction = Osnet\n",
    "        )\n",
    "        print(f\"Annotations ready for video: {full_video_path}\")\n",
    "\n",
    "        # production of the visual output\n",
    "        draw_bbox_from_file(\n",
    "            file_path = f\"{raw_text_output_directory}/{video_name}.txt\", \n",
    "            input_video_path = full_video_path, \n",
    "            output_video_path = f\"{output_video_directory_temp}/{video_name}-(temp).mp4\",\n",
    "            annotation_type=\"bbox\",\n",
    "            draw_frame_count=True\n",
    "        )\n",
    "        print(f\"Treatment done: {full_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Third step:</h2>\n",
    "<h3>Processing the output of the two first steps to produce a textual output (stored in <i>output_text_file_path</i>) and a visual output (accessible via <i>output_edited_video_path</i>).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: the manual annotation file related to the video <C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos\\sample1.MP4> is not found. It must be located at <C:/Users/Theo/Documents/Unif/chimprec-videos/input_videos/manual_annotations/sample1.txt>.\n"
     ]
    }
   ],
   "source": [
    "for input_video in os.listdir(input_video_directory):\n",
    "    if input_video.endswith(\".mp4\") or input_video.endswith(\".MP4\"):\n",
    "        full_video_path = os.path.join(input_video_directory, input_video)\n",
    "        video_name = os.path.splitext(input_video)[0]\n",
    "        annotation_file = f\"{mannual_annotations_directory}/{video_name}.txt\"\n",
    "\n",
    "        raw_reader = raw_tracking_data_reader(f\"{raw_text_output_directory}/{video_name}.txt\")\n",
    "\n",
    "        try:\n",
    "            edit_reader = modification_reader(annotation_file)\n",
    "        except:\n",
    "            print(f\"Error: the manual annotation file related to the video <{full_video_path}> is not found. It must be located at <{annotation_file}>.\")\n",
    "            continue\n",
    "        \n",
    "        metadata_file_path = f\"{output_video_directory}/{video_name}-treated.txt\"\n",
    "        output_video_path = f\"{output_video_directory}/{video_name}-treated.mp4\"\n",
    "        writer = data_writer(metadata_file_path)\n",
    "\n",
    "        # computation of the new metadata file\n",
    "        modified_data = edit_raw_output(raw_reader, edit_reader) \n",
    "\n",
    "        # production of the textual output\n",
    "        writer.write(modified_data)\n",
    "\n",
    "        # production of the visual output\n",
    "        draw_bbox_from_file(\n",
    "            file_path = metadata_file_path, \n",
    "            input_video_path = full_video_path, \n",
    "            output_video_path = output_video_path,\n",
    "            annotation_type=\"triangle\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
