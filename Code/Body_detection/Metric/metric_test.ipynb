{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test set path</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric_lib import *\n",
    "test_set = \"/home/theo/Documents/Unif/Master/Chimprec - Extra/Detection - test set\" #insert path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generic function to compute intersection over union.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46731805220968897\n",
      "0.46922246572820714\n",
      "0.4679501297992655\n",
      "0.4777199233072285\n"
     ]
    }
   ],
   "source": [
    "print(iou([0.386891, 0.573836, 0.274225, 0.328799], [0.324225, 0.497304, 0.400000, 0.482353])) # BIG\n",
    "print(iou([0.670643, 0.656066, 0.136706, 0.164583], [0.639254, 0.618382, 0.199631, 0.240196])) # MEDIUM\n",
    "print(iou([0.820532, 0.697243, 0.068833, 0.082230], [0.805022, 0.678125, 0.100000, 0.120956])) # SMALL\n",
    "print(iou([0.902880, 0.717892, 0.033973, 0.040931], [0.895362, 0.709050, 0.049453, 0.058860])) # TINY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46785733580791994\n",
      "0.5542859293230449\n",
      "0.6885386459194439\n",
      "0.749347676802334\n"
     ]
    }
   ],
   "source": [
    "print(weighted_iou([0.386891, 0.573836, 0.274225, 0.328799], [0.324225, 0.497304, 0.400000, 0.482353])) # BIG\n",
    "print(weighted_iou([0.670643, 0.656066, 0.136706, 0.164583], [0.639254, 0.618382, 0.199631, 0.240196])) # MEDIUM\n",
    "print(weighted_iou([0.820532, 0.697243, 0.068833, 0.082230], [0.805022, 0.678125, 0.100000, 0.120956])) # SMALL\n",
    "print(weighted_iou([0.902880, 0.717892, 0.033973, 0.040931], [0.895362, 0.709050, 0.049453, 0.058860])) # TINY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30924394992000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_covered([0.267645, 0.307458, 0.523305, 0.590944], (1080, 1920))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:green;\">Test of the <i>iou</i> function.</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(observed, expected, test_name=\"\"):\n",
    "    msg = f'({test_name}): Observed: {observed}; Expected: {expected}'\n",
    "    if (round(observed, 5) == round(expected, 5)): print(f\"Test passed ({test_name})\")\n",
    "    else: print(f\"Test failed{msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed (Perfect Overlap)\n",
      "Test passed (No Overlap)\n",
      "Test passed (No Overlap)\n"
     ]
    }
   ],
   "source": [
    "bbox = [0.1, 0.1, 0.2, 0.2]\n",
    "test(iou(bbox, bbox), 1, \"Perfect Overlap\") # perfect overlap\n",
    "\n",
    "bbox2 = [0.9, 0.9, 0.1, 0.1] \n",
    "test(iou(bbox, bbox2), 0, \"No Overlap\") # no overlap\n",
    "\n",
    "bbox3 = [0.1, 0.2, 0.2, 0.2] \n",
    "test(iou(bbox, bbox3), 1/3, \"No Overlap\") # no overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computation of the following metrics based on the ground truth (from the test set) and the predictions</h2>\n",
    "<h3>\n",
    "    <ul>\n",
    "        <li>True Positive</li>\n",
    "        <li>False Positive</li>\n",
    "        <li><span style=\"color:grey;\">True Negative: not counted (see below)</span></li>\n",
    "        <li>False Negative</li>\n",
    "    </ul> \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:green;\">Test of the <i>extract_metrics</i> function.</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "Test passed (perfect match (TP))\n",
      "Test passed (perfect match (FP))\n",
      "Test passed (perfect match (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (No overlap (TP))\n",
      "Test passed (No overlap (FP))\n",
      "Test passed (No overlap (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (No overlap but bboxes are closer (TP))\n",
      "Test passed (No overlap but bboxes are closer (FP))\n",
      "Test passed (No overlap but bboxes are closer (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Not a perfect match but above threshold (TP))\n",
      "Test passed (Not a perfect match but above threshold (FP))\n",
      "Test passed (Not a perfect match but above threshold (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Two prediction bboxes matching the same ground truth (TP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Two prediction bboxes matching the same ground truth (TP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Two prediction bboxes matching the same ground truth (TP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (GT completed included within prediction (TP))\n",
      "Test passed (GT completed included within prediction (FP))\n",
      "Test passed (GT completed included within prediction (FN))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------------------------\")\n",
    "# perfect match\n",
    "GT = {\"img1\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "PRED = {\"img1\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"perfect match (TP)\")\n",
    "test(fp, 0, \"perfect match (FP)\")\n",
    "test(fn, 0, \"perfect match (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# No overlap\n",
    "GT = {\"img2\": [[0.8, 0.8, 0.1, 0.1]]}\n",
    "PRED = {\"img2\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 0, \"No overlap (TP)\")\n",
    "test(fp, 1, \"No overlap (FP)\")\n",
    "test(fn, 1, \"No overlap (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# No overlap but bboxes are closer\n",
    "GT = {\"img3\": [[0.1, 0.1, 0.1, 0.1]]}\n",
    "PRED = {\"img3\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 0, \"No overlap but bboxes are closer (TP)\")\n",
    "test(fp, 1, \"No overlap but bboxes are closer (FP)\")\n",
    "test(fn, 1, \"No overlap but bboxes are closer (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# Not a perfect match but above threshold\n",
    "GT = {\"img4\": [[0.1, 0.1, 0.1, 0.1]]}\n",
    "PRED = {\"img4\": [[0.1, 0.1, 0.09, 0.09]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"Not a perfect match but above threshold (TP)\")\n",
    "test(fp, 0, \"Not a perfect match but above threshold (FP)\")\n",
    "test(fn, 0, \"Not a perfect match but above threshold (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# Two prediction bboxes matching the same ground truth\n",
    "GT = {\"img5\": [[0.1, 0.1, 0.1, 0.1]]}\n",
    "PRED = {\"img5\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"Two prediction bboxes matching the same ground truth (TP)\")\n",
    "test(fp, 1, \"Two prediction bboxes matching the same ground truth (FP)\")\n",
    "test(fn, 0, \"Two prediction bboxes matching the same ground truth (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# Composed case - 2 pred matching 1 GT - 1 pred matching 1 GT\n",
    "GT = {\"img6\": [[0.1, 0.1, 0.1, 0.1], [0.8, 0.8, 0.2, 0.3]]}\n",
    "PRED = {\"img6\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09], [0.8, 0.8, 0.18, 0.32]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 2, \"Two prediction bboxes matching the same ground truth (TP)\")\n",
    "test(fp, 1, \"Two prediction bboxes matching the same ground truth (FP)\")\n",
    "test(fn, 0, \"Two prediction bboxes matching the same ground truth (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# One prediction overlapping two ground truths\n",
    "GT = {\"img7\": [[0.269316, 0.482907, 0.351723, 0.377785], [0.369253, 0.815242, 0.354316, 0.346540]]}\n",
    "PRED = {\"img7\": [[0.367304, 0.808140, 0.362103, 0.366419]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"Two prediction bboxes matching the same ground truth (TP)\")\n",
    "test(fp, 0, \"Two prediction bboxes matching the same ground truth (FP)\")\n",
    "test(fn, 1, \"Two prediction bboxes matching the same ground truth (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# GT completed included within prediction\n",
    "GT = {\"img8\": [[0.5, 0.5, 0.5, 0.5]]}\n",
    "PRED = {\"img8\": [[0.5, 0.5, 0.55, 0.55]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"GT completed included within prediction (TP)\")\n",
    "test(fp, 0, \"GT completed included within prediction (FP)\")\n",
    "test(fn, 0, \"GT completed included within prediction (FN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:green;\">Combined test of the <i>extract_metrics</i> function.</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed (Combined test (TP))\n",
      "Test passed (Combined test (FP))\n",
      "Test passed (Combined test (FN))\n"
     ]
    }
   ],
   "source": [
    "GT = {\n",
    "    \"img1\": [[0.2, 0.2, 0.1, 0.1]], \n",
    "    \"img2\": [[0.8, 0.8, 0.1, 0.1]], \n",
    "    \"img3\": [[0.1, 0.1, 0.1, 0.1]],\n",
    "    \"img4\": [[0.1, 0.1, 0.1, 0.1]],\n",
    "    \"img5\": [[0.1, 0.1, 0.1, 0.1]],\n",
    "    \"img6\": [[0.1, 0.1, 0.1, 0.1], [0.8, 0.8, 0.2, 0.3]],\n",
    "    \"img7\": [[0.269316, 0.482907, 0.351723, 0.377785], [0.369253, 0.815242, 0.354316, 0.346540]]\n",
    "    }\n",
    "PRED = {\n",
    "    \"img1\": [[0.2, 0.2, 0.1, 0.1]],    # +1 TP\n",
    "    \"img2\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img3\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img4\": [[0.1, 0.1, 0.09, 0.09]],  # +1 TP --> not a perfect match but above threshold\n",
    "    \"img5\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09]], # +1 TP, +1 FP\n",
    "    \"img6\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09], [0.8, 0.8, 0.18, 0.32]], # +2 TP, +1 FP\n",
    "    \"img7\": [[0.367304, 0.808140, 0.362103, 0.366419]] # +1 TP, +1 FN\n",
    "    }\n",
    "\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 6, \"Combined test (TP)\")\n",
    "test(fp, 4, \"Combined test (FP)\")\n",
    "test(fn, 3, \"Combined test (FN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract the data from the test set (to <i>dict</i>)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:green;\">Test of the <i>extract_ground_truth</i> function.</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hh\n",
      "Test passed (Extract ground truth from files (TP))\n",
      "Test passed (Extract ground truth from files (FP))\n",
      "Test passed (Extract ground truth from files (FN))\n"
     ]
    }
   ],
   "source": [
    "GT = extract_ground_truth(test_set_path=\"test_set_example\")\n",
    "PRED = {\n",
    "    \"img1\": [[0.2, 0.2, 0.1, 0.1]],    # +1 TP\n",
    "    \"img2\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img3\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img4\": [[0.1, 0.1, 0.09, 0.09]],  # +1 TP --> not a perfect match but above threshold\n",
    "    \"img5\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09]], # +1 TP, +1 FP\n",
    "    \"img6\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09], [0.8, 0.8, 0.18, 0.32]], # +2 TP, +1 FP\n",
    "    \"img7\": [[0.367304, 0.808140, 0.362103, 0.366419]] # +1 TP, +1 FN\n",
    "    }\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 6, \"Extract ground truth from files (TP)\")\n",
    "test(fp, 4, \"Extract ground truth from files (FP)\")\n",
    "test(fn, 3, \"Extract ground truth from files (FN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Effective code</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ground truth extraction.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hh\n"
     ]
    }
   ],
   "source": [
    "GT = extract_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predictions extraction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualisation functions.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\home\\\\theo\\\\Documents\\\\Unif\\\\Master\\\\ChimpRec\\\\Code\\\\Body_detection\\\\YOLO_small\\\\runs\\\\detect\\\\train9\\\\weights\\\\best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m             predictions[filename_prefix]\u001b[38;5;241m.\u001b[39mappend((x_center, y_center, width, height))\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[1;32m---> 63\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model_path, test_set_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load the YOLO model\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m test_set_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_set_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Ensure the test set path exists\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\ultralytics\\engine\\model.py:148\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\ultralytics\\engine\\model.py:290\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    287\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:1039\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m    Load a single model weights.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m        (tuple): Tuple containing the model and checkpoint.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:944\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight, safe_only)\u001b[0m\n\u001b[0;32m    942\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 944\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _torch_load(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Theo\\Documents\\Unif\\ChimpRec\\Code\\Body_detection\\Metric\\.venv\\lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\home\\\\theo\\\\Documents\\\\Unif\\\\Master\\\\ChimpRec\\\\Code\\\\Body_detection\\\\YOLO_small\\\\runs\\\\detect\\\\train9\\\\weights\\\\best.pt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"/home/theo/Documents/Unif/Master/ChimpRec/Code/Body_detection/YOLO_small/runs/detect/train9/weights/best.pt\"\n",
    "\n",
    "def predict(model_path, test_set_path):\n",
    "    \"\"\"\n",
    "    Predict bounding boxes using a YOLO model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the YOLO model.\n",
    "        test_set_path (str): Path to the test set directory.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with image prefixes as keys and predicted bounding boxes (YOLO format).\n",
    "    \"\"\"\n",
    "    predictions = dict()\n",
    "\n",
    "    # Load the YOLO model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    test_set_path = f\"{test_set_path}/images\"\n",
    "\n",
    "    # Ensure the test set path exists\n",
    "    if not os.path.isdir(test_set_path):\n",
    "        print(f\"Error: Test set directory '{test_set_path}' not found.\")\n",
    "        return {}\n",
    "\n",
    "    # Iterate over all images in the test set directory\n",
    "    image_extension = \".png\"\n",
    "    for filename in sorted(os.listdir(test_set_path)):\n",
    "        file_path = os.path.join(test_set_path, filename)\n",
    "\n",
    "        # Check if the file is an image\n",
    "        if not filename.lower().endswith(image_extension):\n",
    "            continue\n",
    "\n",
    "        filename_prefix = filename.split(\".\")[0]\n",
    "        predictions[filename_prefix] = []\n",
    "\n",
    "        # Run inference\n",
    "        results = model(file_path)[0]\n",
    "\n",
    "        # Extract YOLO format bounding boxes\n",
    "        img = cv2.imread(file_path)\n",
    "        img_height, img_width, _ = img.shape\n",
    "\n",
    "        for result in results.boxes.data.tolist():\n",
    "            x1, y1, x2, y2, score, class_id = result\n",
    "\n",
    "            # Convert bbox to YOLO format (normalized)\n",
    "            x_center = ((x1 + x2) / 2) / img_width\n",
    "            y_center = ((y1 + y2) / 2) / img_height\n",
    "            width = (x2 - x1) / img_width\n",
    "            height = (y2 - y1) / img_height\n",
    "\n",
    "            predictions[filename_prefix].append((x_center, y_center, width, height))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model_path, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualisation support</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_predictions(predictions, ground_truth, test_set_path):\n",
    "    \"\"\"\n",
    "    Draws predicted and ground truth bounding boxes on images and displays them in Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): Dictionary of predicted bounding boxes in YOLO format.\n",
    "        ground_truth (dict): Dictionary of ground truth bounding boxes in YOLO format.\n",
    "        test_set_path (str): Path to the test set directory.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    test_set_path = f\"{test_set_path}/images\"\n",
    "\n",
    "    for img_prefix, pred_bboxes in predictions.items():\n",
    "        img_name = f\"{img_prefix}.png\"\n",
    "        img_path = os.path.join(test_set_path, img_name)\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Unable to load image {img_name}\")\n",
    "            continue\n",
    "\n",
    "        # Convert from BGR to RGB for correct color display\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img_height, img_width, _ = image.shape\n",
    "\n",
    "        def draw_bbox(image, bbox, color, label=None):\n",
    "            \"\"\" Draws a single bounding box on the image. \"\"\"\n",
    "            print(bbox)\n",
    "            x_center, y_center, width, height = bbox\n",
    "\n",
    "            # Convert YOLO format to absolute pixel coordinates\n",
    "            x1 = int((x_center - width / 2) * img_width)\n",
    "            y1 = int((y_center - height / 2) * img_height)\n",
    "            x2 = int((x_center + width / 2) * img_width)\n",
    "            y2 = int((y_center + height / 2) * img_height)\n",
    "\n",
    "            # Draw rectangle\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "            # Put label\n",
    "            if label:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 0.5\n",
    "                thickness = 1\n",
    "                cv2.putText(image, label, (x1, max(y1 - 5, 10)), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        # Draw ground truth boxes (green)\n",
    "        if img_prefix in ground_truth:\n",
    "            for bbox in ground_truth[img_prefix]:\n",
    "                print(bbox)\n",
    "                draw_bbox(image, bbox, (0, 255, 0))  # Class label only\n",
    "\n",
    "        # Draw predicted boxes (blue)\n",
    "        for bbox in pred_bboxes:\n",
    "            draw_bbox(image, bbox, (255, 0, 0), f\"P {int(bbox[0])}\")  # Class & confidence\n",
    "\n",
    "        # Display image\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Predictions & Ground Truth for {img_name}\")\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# predictions = predict(model_path, test_set)\n",
    "draw_predictions(predictions, GT, test_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computation of the model performance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_positives': 718, 'false_positives': 262, 'false_negatives': 475}\n",
      "precision=0.7326530612244898\n",
      "recall=0.6018440905280805\n"
     ]
    }
   ],
   "source": [
    "print(extract_metrics(GT, predictions, t=0.6))\n",
    "\n",
    "tp, fp, fn = extract_metrics(GT, predictions, t=0.6).values()\n",
    "print(f\"precision={tp/(tp+fp)}\")\n",
    "print(f\"recall={tp/(tp+fn)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
