{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = \"...\"\n",
    "model_path = \"...\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/Theo/Documents/Unif/ChimpRec/Code\")\n",
    "from chimplib.metric import extract_ground_truth, extract_metrics, predict, merge_boxes, draw_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ground truth extraction.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.6\n",
    "t_confidence = 0.35\n",
    "merging_threshold = 0.8\n",
    "\n",
    "GT = extract_ground_truth()\n",
    "predictions = predict(model_path, test_set, t_confidence=t_confidence)\n",
    "merged_predictions = merge_boxes(predictions, merging_threshold)\n",
    "#predictions = merged_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computation of the model performance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_positives': 702, 'false_positives': 202, 'false_negatives': 148}\n",
      "precision=0.7765486725663717\n",
      "recall=0.8258823529411765\n"
     ]
    }
   ],
   "source": [
    "results = extract_metrics(GT, predictions, t=t)\n",
    "\n",
    "tp, fp, fn = results.values()\n",
    "\n",
    "print(results)\n",
    "print(f\"precision={tp/(tp+fp)}\")\n",
    "print(f\"recall={tp/(tp+fn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Graphs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Precision and recall as a function of the confidence threshold\n",
    "\"\"\"\n",
    "iterators = [i/20 for i in range(1, 20, 1)]\n",
    "\n",
    "data = dict()\n",
    "n_pred_list = []\n",
    "\n",
    "for i in iterators:\n",
    "    pred = predict(model_path, test_set, i)\n",
    "    results = extract_metrics(GT, pred, t=t)\n",
    "    tp, fp, fn = results.values()\n",
    "    if (tp+fp) == 0: precision = 0\n",
    "    else: precision=tp/(tp+fp)\n",
    "\n",
    "    if (tp+fn) == 0: recall = 0\n",
    "    else: recall=tp/(tp+fn)\n",
    "    data[i] = (precision, recall)\n",
    "    n_pred = 0\n",
    "    for values in pred.values():\n",
    "        n_pred += len(values)\n",
    "    n_pred_list.append(n_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the predictions and save the images in the visualisation file \n",
    "draw_predictions(predictions, GT, test_set, \"visualisation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
