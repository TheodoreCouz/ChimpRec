{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test set path</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = \"C:/Users/Theo/Documents/Unif/detection test set\" #insert path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generic function to compute intersection over union.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_to_relative_coord(bbox, img_dim):\n",
    "    x_center, y_center, width, height = bbox\n",
    "    img_w, img_h = img_dim\n",
    "    \n",
    "    x_min = (x_center - width / 2) * img_w\n",
    "    y_min = (y_center - height / 2) * img_h\n",
    "    x_max = (x_center + width / 2) * img_w\n",
    "    y_max = (y_center + height / 2) * img_h\n",
    "    \n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input:\n",
    "bbox1, bbox2: bounding boxes in YOLO format\n",
    "YOLO format ==> [x_center, y_center, width, height], normailzed coordinates\n",
    "\n",
    "output:\n",
    "The intersection over union metric between bbox1 and bbox2\n",
    "\"\"\"\n",
    "def iou(bbox1, bbox2, img_dim=(1080, 1920)):\n",
    "\n",
    "    # conversion into relative coordinates\n",
    "    Ax, Ay, Bx, By = yolo_to_relative_coord(bbox1, img_dim)\n",
    "    Cx, Cy, Dx, Dy = yolo_to_relative_coord(bbox2, img_dim)\n",
    "\n",
    "    # computation of the intersection\n",
    "    x_overlap = min(Dx, Bx) - max(Ax, Cx)\n",
    "    y_overlap = min(Dy, By) - max(Ay, Cy)\n",
    "    if (x_overlap < 0 or y_overlap < 0): return 0 # no overlap case\n",
    "    intersection = x_overlap*y_overlap\n",
    "\n",
    "    # computation of the union\n",
    "    area_1 = abs((Bx-Ax)*(By-Ay))\n",
    "    area_2 = abs((Dx-Cx)*(Dy-Cy))\n",
    "    union = area_1 + area_2 - intersection\n",
    "\n",
    "    # IoU\n",
    "    return intersection/union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test of the <i>iou</i> function.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(observed, expected, test_name=\"\"):\n",
    "    msg = f'({test_name}): Observed: {observed}; Expected: {expected}'\n",
    "    if (round(observed, 5) == round(expected, 5)): print(f\"Test passed ({test_name})\")\n",
    "    else: print(f\"Test failed{msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed (Perfect Overlap)\n",
      "Test passed (No Overlap)\n",
      "Test passed (No Overlap)\n"
     ]
    }
   ],
   "source": [
    "bbox = [0.1, 0.1, 0.2, 0.2]\n",
    "test(iou(bbox, bbox), 1, \"Perfect Overlap\") # perfect overlap\n",
    "\n",
    "bbox2 = [0.9, 0.9, 0.1, 0.1] \n",
    "test(iou(bbox, bbox2), 0, \"No Overlap\") # no overlap\n",
    "\n",
    "bbox3 = [0.1, 0.2, 0.2, 0.2] \n",
    "test(iou(bbox, bbox3), 1/3, \"No Overlap\") # no overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computation of the following metrics based on the ground truth (from the test set) and the predictions</h2>\n",
    "<h3>\n",
    "    <ul>\n",
    "        <li>True Positive</li>\n",
    "        <li>False Positive</li>\n",
    "        <li><span style=\"color:grey;\">True Negative: not counted (see below)</span></li>\n",
    "        <li>False Negative</li>\n",
    "    </ul> \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The arguments <ground_truths> and <predictions> are expected to be \n",
    "python dictionaries structured as follows:\n",
    "\n",
    "    dictionnary {\n",
    "        image_name 1: [\n",
    "                bbox 1,\n",
    "                bbox 2, \n",
    "                ...\n",
    "            ],\n",
    "        image_name 2: [\n",
    "                bbox 1,\n",
    "                bbox 2, \n",
    "                ...\n",
    "            ],\n",
    "        ...\n",
    "    }\n",
    "\n",
    "PS: the bounding boxes are expected to be encoded in YOLO format.\n",
    "\n",
    "    \n",
    "For each image, every prediction bbox will be \n",
    "compared to every ground truth bbox to find the best match.\n",
    "\n",
    "If a ground truth bbox finds no match among the prediction bbox:\n",
    "+1 false negative\n",
    "\n",
    "If a prediction bbox finds no match among the ground truth bbox:\n",
    "+1 false positive\n",
    "\n",
    "If the best match between a prediction bbox and the ground truth\n",
    "bboxes has a low IoU (bellow a given threshold <t>):\n",
    "+1 false positive\n",
    "\n",
    "If a prediction bbox best match in the ground truth bboxes has\n",
    "already been matched by another prediction bbox with a greater score:\n",
    "+1 false positive\n",
    "\n",
    "If the best match between a prediction bbox and the ground truth\n",
    "bboxes has a high IoU (above a given threshold <t>):\n",
    "+1 true positive\n",
    "\n",
    "The true negative won't be counted because it means that no\n",
    "prediction bbox has been found in the background. This is nonsense to count this.\n",
    "\"\"\"\n",
    "def extract_metrics(ground_truths:dict, predictions:dict, t=0.75):\n",
    "    tp, fp, fn = 0, 0, 0  # Initialize counters for TP, FP, and FN\n",
    "    \n",
    "    # Iterate over all unique image names in ground truths and predictions\n",
    "    for image_name in set(ground_truths.keys()).union(predictions.keys()):\n",
    "        gt_bboxes = ground_truths.get(image_name, [])  # Retrieve ground truth bboxes (default empty list if missing)\n",
    "        pred_bboxes = predictions.get(image_name, [])  # Retrieve predicted bboxes (default empty list if missing)\n",
    "        \n",
    "        matched_gt = set()  # Store indices of matched ground truth bboxes\n",
    "        pred_matched_scores = []  # Keep track of IoU scores of matched predictions\n",
    "        \n",
    "        # Iterate over each predicted bounding box\n",
    "        for pred in pred_bboxes:\n",
    "            best_iou = 0  # Initialize the best IoU score for the current prediction\n",
    "            best_gt_idx = -1  # Index of the best-matching ground truth bbox\n",
    "            \n",
    "            # Compare prediction with each ground truth bbox\n",
    "            for i, gt in enumerate(gt_bboxes):\n",
    "                score = iou(pred, gt)  # Compute IoU\n",
    "                if score > best_iou:  # Update best match if IoU is higher\n",
    "                    best_iou = score\n",
    "                    best_gt_idx = i\n",
    "            \n",
    "            # Determine if the prediction is a TP or FP based on IoU and previous matches\n",
    "            if best_iou >= t and best_gt_idx not in matched_gt:\n",
    "                matched_gt.add(best_gt_idx)  # Mark ground truth bbox as matched\n",
    "                pred_matched_scores.append(best_iou)  # Store IoU score\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        \n",
    "        # Count False Negatives (ground truths that were not matched)\n",
    "        fn += len(gt_bboxes) - len(matched_gt)\n",
    "    \n",
    "    return {\"true_positives\": tp, \"false_positives\": fp, \"false_negatives\": fn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test of the <i>extract_metrics</i> function.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "Test passed (perfect match (TP))\n",
      "Test passed (perfect match (FP))\n",
      "Test passed (perfect match (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (No overlap (TP))\n",
      "Test passed (No overlap (FP))\n",
      "Test passed (No overlap (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (No overlap but bboxes are closer (TP))\n",
      "Test passed (No overlap but bboxes are closer (FP))\n",
      "Test passed (No overlap but bboxes are closer (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Not a perfect match but above threshold (TP))\n",
      "Test passed (Not a perfect match but above threshold (FP))\n",
      "Test passed (Not a perfect match but above threshold (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Two prediction bboxes matching the same ground truth (TP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Two prediction bboxes matching the same ground truth (TP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FN))\n",
      "\n",
      "------------------------------------\n",
      "Test passed (Two prediction bboxes matching the same ground truth (TP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FP))\n",
      "Test passed (Two prediction bboxes matching the same ground truth (FN))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------------------------------\")\n",
    "# perfect match\n",
    "GT = {\"img1\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "PRED = {\"img1\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"perfect match (TP)\")\n",
    "test(fp, 0, \"perfect match (FP)\")\n",
    "test(fn, 0, \"perfect match (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# No overlap\n",
    "GT = {\"img2\": [[0.8, 0.8, 0.1, 0.1]]}\n",
    "PRED = {\"img2\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 0, \"No overlap (TP)\")\n",
    "test(fp, 1, \"No overlap (FP)\")\n",
    "test(fn, 1, \"No overlap (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# No overlap but bboxes are closer\n",
    "GT = {\"img3\": [[0.1, 0.1, 0.1, 0.1]]}\n",
    "PRED = {\"img3\": [[0.2, 0.2, 0.1, 0.1]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 0, \"No overlap but bboxes are closer (TP)\")\n",
    "test(fp, 1, \"No overlap but bboxes are closer (FP)\")\n",
    "test(fn, 1, \"No overlap but bboxes are closer (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# Not a perfect match but above threshold\n",
    "GT = {\"img4\": [[0.1, 0.1, 0.1, 0.1]]}\n",
    "PRED = {\"img4\": [[0.1, 0.1, 0.09, 0.09]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"Not a perfect match but above threshold (TP)\")\n",
    "test(fp, 0, \"Not a perfect match but above threshold (FP)\")\n",
    "test(fn, 0, \"Not a perfect match but above threshold (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# Two prediction bboxes matching the same ground truth\n",
    "GT = {\"img5\": [[0.1, 0.1, 0.1, 0.1]]}\n",
    "PRED = {\"img5\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"Two prediction bboxes matching the same ground truth (TP)\")\n",
    "test(fp, 1, \"Two prediction bboxes matching the same ground truth (FP)\")\n",
    "test(fn, 0, \"Two prediction bboxes matching the same ground truth (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# Composed case - 2 pred matching 1 GT - 1 pred matching 1 GT\n",
    "GT = {\"img6\": [[0.1, 0.1, 0.1, 0.1], [0.8, 0.8, 0.2, 0.3]]}\n",
    "PRED = {\"img6\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09], [0.8, 0.8, 0.18, 0.32]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 2, \"Two prediction bboxes matching the same ground truth (TP)\")\n",
    "test(fp, 1, \"Two prediction bboxes matching the same ground truth (FP)\")\n",
    "test(fn, 0, \"Two prediction bboxes matching the same ground truth (FN)\")\n",
    "\n",
    "print(\"\\n------------------------------------\")\n",
    "# One prediction overlapping two ground truths\n",
    "GT = {\"img7\": [[0.269316, 0.482907, 0.351723, 0.377785], [0.369253, 0.815242, 0.354316, 0.346540]]}\n",
    "PRED = {\"img7\": [[0.367304, 0.808140, 0.362103, 0.366419]]}\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 1, \"Two prediction bboxes matching the same ground truth (TP)\")\n",
    "test(fp, 0, \"Two prediction bboxes matching the same ground truth (FP)\")\n",
    "test(fn, 1, \"Two prediction bboxes matching the same ground truth (FN)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combined test of the <i>extract_metrics</i> function.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed (Combined test (TP))\n",
      "Test passed (Combined test (FP))\n",
      "Test passed (Combined test (FN))\n"
     ]
    }
   ],
   "source": [
    "GT = {\n",
    "    \"img1\": [[0.2, 0.2, 0.1, 0.1]], \n",
    "    \"img2\": [[0.8, 0.8, 0.1, 0.1]], \n",
    "    \"img3\": [[0.1, 0.1, 0.1, 0.1]],\n",
    "    \"img4\": [[0.1, 0.1, 0.1, 0.1]],\n",
    "    \"img5\": [[0.1, 0.1, 0.1, 0.1]],\n",
    "    \"img6\": [[0.1, 0.1, 0.1, 0.1], [0.8, 0.8, 0.2, 0.3]],\n",
    "    \"img7\": [[0.269316, 0.482907, 0.351723, 0.377785], [0.369253, 0.815242, 0.354316, 0.346540]]\n",
    "    }\n",
    "PRED = {\n",
    "    \"img1\": [[0.2, 0.2, 0.1, 0.1]],    # +1 TP\n",
    "    \"img2\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img3\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img4\": [[0.1, 0.1, 0.09, 0.09]],  # +1 TP --> not a perfect match but above threshold\n",
    "    \"img5\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09]], # +1 TP, +1 FP\n",
    "    \"img6\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09], [0.8, 0.8, 0.18, 0.32]], # +2 TP, +1 FP\n",
    "    \"img7\": [[0.367304, 0.808140, 0.362103, 0.366419]] # +1 TP, +1 FN\n",
    "    }\n",
    "\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 6, \"Combined test (TP)\")\n",
    "test(fp, 4, \"Combined test (FP)\")\n",
    "test(fn, 3, \"Combined test (FN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract the data from the test set (to <i>dict</i>)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def extract_ground_truth(test_set_path = test_set):\n",
    "    path = f'{test_set_path}/labels/obj_train_data'\n",
    "    data = dict()\n",
    "\n",
    "    # check whether the specified path is valid directory\n",
    "    if not os.path.isdir(path):\n",
    "        print(f\"Error: {path} is not a valid directory.\")\n",
    "        return\n",
    "    \n",
    "    for textfile in os.listdir(path):\n",
    "        file_path = f\"{path}/{textfile}\"\n",
    "\n",
    "        # check whether textfile exists in the folder located at the specified path\n",
    "        if os.path.isfile(file_path):\n",
    "            data[textfile.strip(\".txt\")] = []\n",
    "\n",
    "            # open the file in read mode\n",
    "            with open(file_path, 'r') as file:\n",
    "\n",
    "                # extract the bounding boxes one by one\n",
    "                for line in file.readlines():\n",
    "                    splitted = line.split(\" \")\n",
    "                    detection_class = splitted[0]\n",
    "                    if (detection_class == 0): continue # abort iteration if the class is a face\n",
    "                    bbox = splitted[1:]\n",
    "                    for i in range(len(bbox)): bbox[i] = float(bbox[i])\n",
    "                    data[textfile.strip(\".txt\")].append(bbox)\n",
    "                file.close()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test of the <i>extract_ground_truth</i> function.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed (Extract ground truth from files (TP))\n",
      "Test passed (Extract ground truth from files (FP))\n",
      "Test passed (Extract ground truth from files (FN))\n",
      "{'img1': [[0.2, 0.2, 0.1, 0.1]], 'img2': [[0.8, 0.8, 0.1, 0.1]], 'img3': [[0.1, 0.1, 0.1, 0.1]], 'img4': [[0.1, 0.1, 0.1, 0.1]], 'img5': [[0.1, 0.1, 0.1, 0.1]], 'img6': [[0.1, 0.1, 0.1, 0.1], [0.8, 0.8, 0.2, 0.3]], 'img7': [[0.269316, 0.482907, 0.351723, 0.377785], [0.369253, 0.815242, 0.354316, 0.34654]]}\n"
     ]
    }
   ],
   "source": [
    "GT = extract_ground_truth(test_set_path=\"test_set_example\")\n",
    "PRED = {\n",
    "    \"img1\": [[0.2, 0.2, 0.1, 0.1]],    # +1 TP\n",
    "    \"img2\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img3\": [[0.2, 0.2, 0.1, 0.1]],    # +1 FP, +1 FN\n",
    "    \"img4\": [[0.1, 0.1, 0.09, 0.09]],  # +1 TP --> not a perfect match but above threshold\n",
    "    \"img5\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09]], # +1 TP, +1 FP\n",
    "    \"img6\": [[0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.09, 0.09], [0.8, 0.8, 0.18, 0.32]], # +2 TP, +1 FP\n",
    "    \"img7\": [[0.367304, 0.808140, 0.362103, 0.366419]] # +1 TP, +1 FN\n",
    "    }\n",
    "tp, fp, fn = extract_metrics(GT, PRED).values()\n",
    "test(tp, 6, \"Extract ground truth from files (TP)\")\n",
    "test(fp, 4, \"Extract ground truth from files (FP)\")\n",
    "test(fn, 3, \"Extract ground truth from files (FN)\")\n",
    "\n",
    "print(GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Effective code</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ground truth extraction.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT = extract_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predictions extraction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Theo/Documents/Unif/ChimpRec/Code/Body_detection/YOLO_small/runs/detect/train9/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"C:/Users/Theo/Documents/Unif/ChimpRec/Code/Body_detection/YOLO_small/runs/detect/train9/weights/best.pt\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
