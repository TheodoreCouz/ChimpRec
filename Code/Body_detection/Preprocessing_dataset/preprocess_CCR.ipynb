{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/Theo/Documents/Unif/ChimpRec/Code')\n",
    "\n",
    "from chimplib.imports import pd, cv2, os, random, math, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = \"...\" # path to labels\n",
    "videos_path = \"...\" #path to videos\n",
    "\n",
    "dataset_path = \"...\" # output dataset path\n",
    "\n",
    "\n",
    "df = pd.read_csv(annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of the whole dataset that is kept (approximately 1500 images)\n",
    "proportion_kept = 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input: n: size of the sample\n",
    "# proportion: (proportion*100)% of the numbers in [0, n-1]\n",
    "# @outputs\n",
    "# No numbers in common between the outputs\n",
    "def segment_dataset(n, proportion):\n",
    "    # Calculate the number of elements to select based on the proportion\n",
    "    num_elements = int(n * proportion)\n",
    "    if num_elements == 0: return []\n",
    "    \n",
    "    # Generate evenly spaced numbers within the range [0, n-1]\n",
    "    step = n / num_elements\n",
    "    selected_numbers = [int(i * step) for i in range(num_elements)]\n",
    "    \n",
    "    return selected_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input: path to a video\n",
    "# @outputs\n",
    "# Length of the video in terms of frames\n",
    "def get_len_video(path_to_video):\n",
    "    video = cv2.VideoCapture(f\"{videos_path}/{path_to_video}\")\n",
    "\n",
    "    if not video.isOpened():\n",
    "        print(f\"Could not open video: {path_to_video}\")\n",
    "        return 0\n",
    "    \n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video.release()\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input: \n",
    "# coord: the coordinate to be converted\n",
    "# img_width: width of the image (in pixels)\n",
    "# img_height: height of the image (in pixels)\n",
    "# @outputs\n",
    "# coordinates expressed in YOLO format\n",
    "def preprocess_coordinates(coord, img_width, img_height):\n",
    "\n",
    "    x, y, w, h = coord\n",
    "    \n",
    "    x1 = int(x*img_width)\n",
    "    y1 = int(y*img_width)\n",
    "    x2 = int(x*img_width+w*img_width)\n",
    "    y2 = int((y*img_width)+h*img_height)\n",
    "\n",
    "    cx = (x1 + x2) / 2\n",
    "    cy = (y1 + y2) / 2\n",
    "\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    cx_norm = cx / img_width\n",
    "    cy_norm = cy / img_height\n",
    "\n",
    "    width_norm = width / img_width\n",
    "    height_norm = height / img_height\n",
    "\n",
    "    return [cx_norm, cy_norm, width_norm, height_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the annotation data\n",
    "def extract_metadata(video_type, video_id, frame_number, img_dim):\n",
    "    img_W, img_H = img_dim\n",
    "    sample_id = video_id.replace(\".mp4\", \"\")\n",
    "    sub_df = df.loc[(df[\"video\"] == video_id) & (df[\"frame\"] == frame_number)]\n",
    "    output_file = f\"{dataset_path}/labels/{video_type}/{sample_id}_{frame_number}.txt\"\n",
    "\n",
    "    string_output = \"\"\n",
    "\n",
    "    for index, row in sub_df.iterrows():\n",
    "        x, y, w, h = row['x'], row['y'], row['w'], row['h']\n",
    "        cx, cy, W, H = preprocess_coordinates((x, y, w, h), img_W, img_H)\n",
    "        string_output += f\"\\n0 {cx} {cy} {W} {H}\"\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(string_output.strip())\n",
    "    file.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @input:\n",
    "# type: \"train\", \"val\" or \"test\" --> indicates which part of the dataset it is\n",
    "# sample_id: name of the video with the extension\n",
    "# idxs: indexes of the images to keep\n",
    "# @output:\n",
    "# nothing, the image are saved in the output folder\n",
    "def extract_data(type, video_id):\n",
    "\n",
    "    sample_id = video_id[:-4]\n",
    "\n",
    "    output_folder = f\"{dataset_path}/images/{type}\"\n",
    "    video = cv2.VideoCapture(f\"{videos_path}/{video_id}\")\n",
    "    len_video = get_len_video(video_id)\n",
    "\n",
    "    idxs = segment_dataset(len_video, proportion_kept)\n",
    "    idxs = sorted(idxs)\n",
    "\n",
    "    for frame_count in idxs:\n",
    "        df_ = df.loc[df[\"video\"] == video_id]\n",
    "        df_ = df_.loc[df_[\"frame\"] == frame_count] # .loc[df[\"label\"] != \"NOTCHIMP\"]\n",
    "\n",
    "        # Set the video to the specific frame\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "        success, frame = video.read()\n",
    "\n",
    "        if success and df_.size != 0:\n",
    "            frame_filename = f\"{output_folder}/{sample_id}_{frame_count}.jpg\"\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            \n",
    "            # extract the annotations\n",
    "            extract_metadata(type, video_id, frame_count, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(videos_path)\n",
    "n_files = len(filenames)\n",
    "\n",
    "# training proportion\n",
    "train_prop = 0.85\n",
    "\n",
    "numbers = list(range(n_files))\n",
    "random.shuffle(numbers)\n",
    "\n",
    "train_video_idxs = numbers[:math.ceil(train_prop*n_files)]\n",
    "val_video_idxs = numbers[math.ceil(train_prop*n_files):]\n",
    "\n",
    "def process_dataset(video_idxs, type):\n",
    "    # progression bar added\n",
    "    for video_idx in tqdm(video_idxs, desc=f\"Processing videos: {type}\", colour=\"green\"):\n",
    "        video_id = filenames[video_idx]\n",
    "        extract_data(type, video_id)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(train_video_idxs, \"train\")\n",
    "process_dataset(val_video_idxs, \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
