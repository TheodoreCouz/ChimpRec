{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model paths and diverse variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = YOLO(\"/home/theo/Documents/Unif/Master/ChimpRec/Code/Body_detection/YOLO_small/runs/detect/train9/weights/best.pt\")\n",
    "model_2 = YOLO(\"/home/theo/Documents/Unif/Master/ChimpRec/Code/Face_detection/runs/detect/train3/weights/best.pt\")\n",
    "\n",
    "video_path = \"/home/theo/Documents/Unif/Master/Chimprec - Extra/videos/20241023 - 09h28.MP4\"\n",
    "output_path = \"output.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "n = 3  # Process one frame every n frames\n",
    "max_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Limit frame processing for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>First prediction in the pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1(image, t_confidence=0.4):\n",
    "    return tuple(\n",
    "        (x1, y1, x2, y2, score)\n",
    "        for x1, y1, x2, y2, score, _ in model_1.predict(image, verbose=False)[0].boxes.data.tolist()\n",
    "        if score >= t_confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Second prediction in the pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_2(image, t_confidence=0.6):\n",
    "    results = model_2.predict(image, verbose=False)\n",
    "    return max(\n",
    "        ((int(x1), int(y1), int(x2), int(y2), score) for result in results for x1, y1, x2, y2, score, _ in result.boxes.data.tolist()),\n",
    "        default=None, key=lambda x: x[-1] if x[-1] >= t_confidence else float('-inf')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Util functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image, bbox):\n",
    "    x1, y1, x2, y2, _ = map(int, bbox)\n",
    "    return image[max(y1, 0):y2, max(x1, 0):x2]\n",
    "\n",
    "def face_to_src(body_bbox, face_bbox):\n",
    "    bx1, by1, _, _, _ = body_bbox\n",
    "    fx1, fy1, fx2, fy2, score = face_bbox\n",
    "    return (bx1 + fx1, by1 + fy1, bx1 + fx2, by1 + fy2, score)\n",
    "\n",
    "def predict_frame(image):\n",
    "    body_bboxes = predict_1(image)\n",
    "    face_bboxes = tuple(\n",
    "        face_to_src(body_bbox, face_bbox)\n",
    "        for body_bbox in body_bboxes\n",
    "        if (face_bbox := predict_2(crop(image, body_bbox)))\n",
    "    )\n",
    "    return body_bboxes, face_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Main code</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bounding boxes extraction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 7032/7032 [16:18<00:00,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of bboxes: 0.056969 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bboxes = []\n",
    "with tqdm(total=max_frames, desc=\"Processing frames\") as pbar:\n",
    "    for frame_idx in range(max_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        bboxes.append(predict_frame(frame) if frame_idx % n == 0 else bboxes[-1])\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "size_in_mb = sys.getsizeof(bboxes) / (1024 * 1024)\n",
    "print(f\"Memory usage of bboxes: {size_in_mb:.6f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Displaying the bboxes onto the video</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(image, color, bbox, label):\n",
    "    x1, y1, x2, y2, score = bbox\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    factor = 0.65 if label == \"Face\" else 0.3\n",
    "    font_scale = max(0.4, (x2 - x1 + y2 - y1) / 300) * factor\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 4)\n",
    "    label_text = f\"{label}: {score:.2f}\"\n",
    "    (w, h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_COMPLEX, font_scale, 1)\n",
    "\n",
    "    overlay = image.copy()\n",
    "    cv2.rectangle(overlay, (x2 - w - 10, y2 - h - 10), (x2, y2), color, -1)\n",
    "    cv2.addWeighted(overlay, 0.5, image, 0.5, 0, image)\n",
    "\n",
    "    cv2.putText(image, label_text, (x2 - w - 5, y2 - 5), cv2.FONT_HERSHEY_COMPLEX, font_scale, (255,255,255), 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 7032/7032 [02:57<00:00, 39.56it/s]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "max_frames = len(bboxes)\n",
    "\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "with tqdm(total=max_frames, desc=\"Processing frames\") as pbar:\n",
    "    for frame_idx in range(max_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        body_bboxes, face_bboxes = bboxes[frame_idx]\n",
    "\n",
    "        for bbox in body_bboxes:\n",
    "            draw_bbox(frame, (254, 122, 51), bbox, \"Body\")\n",
    "\n",
    "        for bbox in face_bboxes:\n",
    "            draw_bbox(frame, (66, 66, 255), bbox, \"Face\")\n",
    "\n",
    "        out.write(frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
